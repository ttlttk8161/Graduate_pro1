{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Model: Company Data Generation (Merged)\n",
    "\n",
    "이 노트북은 실제 기업 재무 데이터를 기반으로, FT-Transformer+TST 및 Diffusion 모델을 활용하여 합성 기업 시계열 데이터를 생성하는 전체 파이프라인을 구현합니다.\n",
    "\n",
    "이 버전은 `Proposed_Model.ipynb`를 기반으로 하며, `previous_version_Model.ipynb`의 CDF 손실 관련 로직을 통합하여 생성 데이터의 통계적 특성 일치도를 높였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR_DIM_PARAM = 13\n",
    "NUM_YEARS_PARAM = 13 \n",
    "FT_OUT_DIM_PARAM = 16 \n",
    "STOCK_DIM_PARAM = 32\n",
    "DENOISER_D_MODEL = 64\n",
    "\n",
    "EPOCHS_COMPANY_MODEL = 5 # Reduced for quick testing, original was 200\n",
    "EPOCHS_DENOISER_MODEL = 5 # Reduced for quick testing, original was 200\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "LEARNING_RATE_COMPANY = 0.001\n",
    "LEARNING_RATE_DENOISER = 1e-3\n",
    "\n",
    "TENSORBOARD_LOG_DIR_COMPANY = 'runs/company_model_experiment'\n",
    "TENSORBOARD_LOG_DIR_DENOISER = 'runs/denoiser_model_experiment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import rtdl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math # Added for PositionalEncoding\n",
    "from torch.utils.tensorboard import SummaryWriter # TensorBoard import\n",
    "import time # For unique log directory names\n",
    "from torchsort import soft_sort # Added for CDF loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 실행 환경에 따라 디바이스 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_csv = \"data/Table_Data.csv\"  # 실제 파일 경로 설정\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path_csv, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(file_path_csv, encoding='euc-kr')\n",
    "\n",
    "# 불필요한 컬럼 제거 (예: 'Name' 컬럼)\n",
    "if 'Name' in df.columns:\n",
    "    df = df.drop(columns=['Name'], errors='ignore')\n",
    "\n",
    "# 2011~2023년 동안 존재하는 기업만 필터링\n",
    "stock_min_year = df.groupby(\"Stock\")[\"YEAR\"].min()\n",
    "stock_max_year = df.groupby(\"Stock\")[\"YEAR\"].max()\n",
    "\n",
    "valid_stocks_initial = stock_min_year[(stock_min_year == 2011) & (stock_max_year == 2023)].index\n",
    "df_filtered = df[df[\"Stock\"].isin(valid_stocks_initial)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "if df_filtered.empty:\n",
    "    print(\"No companies found that existed continuously from 2011 to 2023. Exiting.\")\n",
    "    # Potentially exit or raise an error if no data to process\n",
    "else:\n",
    "    # 정확히 13년치 데이터가 있는 기업만 선택\n",
    "    year_counts = df_filtered.groupby(\"Stock\")[\"YEAR\"].count()\n",
    "    valid_stocks_final = year_counts[year_counts == NUM_YEARS_PARAM].index\n",
    "    df_filtered = df_filtered[df_filtered[\"Stock\"].isin(valid_stocks_final)].copy()\n",
    "    df_filtered = df_filtered.sort_values(by=[\"Stock\", \"YEAR\"])\n",
    "\n",
    "    print(\"Data before preprocessing:\")\n",
    "    print(f\"Original df shape: {df.shape}\")\n",
    "    print(f\"Number of unique stocks before filtering: {df['Stock'].nunique()}\")\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\nData after preprocessing (filtering):\")\n",
    "    print(f\"Filtered df shape: {df_filtered.shape}\")\n",
    "    print(f\"Number of unique stocks after filtering: {df_filtered['Stock'].nunique()}\")\n",
    "    display(df_filtered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 연속형 & 이진 변수 분리, 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [\"OWN\", \"FORN\", \"SIZE\", \"LEV\", \"CUR\", \"GRW\", \"ROA\", \"ROE\", \"CFO\", \"PPE\", \"AGE\", \"INVREC\", \"MB\", \"TQ\"]\n",
    "binary_features = [\"BIG4\", \"LOSS\"]\n",
    "\n",
    "if not df_filtered.empty:\n",
    "    # Stock 정보를 범주형(정수형)으로 변환\n",
    "    df_filtered.loc[:, \"Stock_ID\"] = df_filtered[\"Stock\"].astype('category').cat.codes\n",
    "\n",
    "    # 연속형 변수 MinMax 정규화\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    scaled_cont = minmax_scaler.fit_transform(\n",
    "        df_filtered[continuous_features]\n",
    "    )\n",
    "\n",
    "    # ② logit(σ⁻¹) 변환 : [0,1] → ℝ\n",
    "    EPS = 1e-6                           # 수치 안정\n",
    "    scaled_cont = np.clip(scaled_cont, EPS, 1.0-EPS)\n",
    "    logit_cont  = np.log(scaled_cont / (1.0 - scaled_cont))\n",
    "\n",
    "    df_filtered.loc[:, continuous_features] = logit_cont\n",
    "\n",
    "    # 이진 변수: 0/1 정수형\n",
    "    df_filtered.loc[:, binary_features] = df_filtered[binary_features].astype(int)\n",
    "\n",
    "    # 전체 feature 목록\n",
    "    features = continuous_features + binary_features\n",
    "\n",
    "    print(\"Data after normalization and transformation:\")\n",
    "    display(df_filtered[features + ['Stock_ID']].head()) # Use display for better notebook output\n",
    "else:\n",
    "    print(\"Skipping normalization and transformation as df_filtered is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 기업 단위 시퀀스 데이터 생성 (각 기업 13년치)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_filtered.empty:\n",
    "    stocks = df_filtered[\"Stock\"].unique()\n",
    "    grouped_cont = []\n",
    "    grouped_bin = []\n",
    "    grouped_year = []\n",
    "    grouped_stock = []\n",
    "\n",
    "    for stock_val in stocks: \n",
    "        df_stock = df_filtered[df_filtered[\"Stock\"] == stock_val].sort_values(by=\"YEAR\")\n",
    "        grouped_cont.append(df_stock[continuous_features].values)  \n",
    "        grouped_bin.append(df_stock[binary_features].values)         \n",
    "        grouped_year.append(df_stock[\"YEAR\"].values)                 \n",
    "        grouped_stock.append(df_stock[\"Stock_ID\"].iloc[0])            \n",
    "\n",
    "    X_cont_seq = np.stack(grouped_cont, axis=0)  \n",
    "    X_bin_seq = np.stack(grouped_bin, axis=0)     \n",
    "    year_seq = np.stack(grouped_year, axis=0)       \n",
    "    stock_seq = np.array(grouped_stock)            \n",
    "\n",
    "    target_seq = np.concatenate([X_cont_seq, X_bin_seq], axis=-1)  \n",
    "\n",
    "    X_cont_tensor = torch.tensor(X_cont_seq, dtype=torch.float32)\n",
    "    X_bin_tensor = torch.tensor(X_bin_seq, dtype=torch.float32)\n",
    "    year_tensor_seq_input = torch.tensor(year_seq, dtype=torch.float32) \n",
    "    stock_tensor_seq_input = torch.tensor(stock_seq, dtype=torch.long) \n",
    "    target_tensor_seq = torch.tensor(target_seq, dtype=torch.float32)\n",
    "\n",
    "    dataset_seq = TensorDataset(X_cont_tensor, X_bin_tensor, year_tensor_seq_input, stock_tensor_seq_input, target_tensor_seq)\n",
    "    dataloader_seq = DataLoader(dataset_seq, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f\"X_cont_tensor shape: {X_cont_tensor.shape}\")\n",
    "    print(f\"X_bin_tensor shape: {X_bin_tensor.shape}\")\n",
    "    print(f\"year_tensor_seq_input shape: {year_tensor_seq_input.shape}\")\n",
    "    print(f\"stock_tensor_seq_input shape: {stock_tensor_seq_input.shape}\")\n",
    "    print(f\"target_tensor_seq shape: {target_tensor_seq.shape}\")\n",
    "    NUM_STOCK_EMBEDDINGS = df_filtered[\"Stock_ID\"].nunique()\n",
    "else:\n",
    "    print(\"Skipping sequence data generation as df_filtered is empty.\")\n",
    "    # Initialize with empty tensors or handle appropriately if needed downstream\n",
    "    X_cont_tensor = torch.empty(0)\n",
    "    X_bin_tensor = torch.empty(0)\n",
    "    year_tensor_seq_input = torch.empty(0)\n",
    "    stock_tensor_seq_input = torch.empty(0)\n",
    "    target_tensor_seq = torch.empty(0)\n",
    "    dataloader_seq = [] # Or an empty DataLoader\n",
    "    NUM_STOCK_EMBEDDINGS = 0 # Default if no stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 연도 임베딩 및 Positional Encoding 함수/클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sine_cosine_year_embedding(years, dim=YEAR_DIM_PARAM):\n",
    "    \"\"\"\n",
    "    - years: (batch, num_years) 또는 (num_samples,) 형태의 실제 연도값 텐서\n",
    "    - 출력: (..., dim) 형태의 연도 임베딩\n",
    "    \"\"\"\n",
    "    if len(years.shape) < 3: \n",
    "        years = years.unsqueeze(-1)\n",
    "        \n",
    "    half_dim = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        torch.arange(0, half_dim, dtype=torch.float32) * (-np.log(10000.0) / half_dim)\n",
    "    ).to(years.device)\n",
    "    sinusoidal_input = years * freqs  \n",
    "    sin_embed = torch.sin(sinusoidal_input)\n",
    "    cos_embed = torch.cos(sinusoidal_input)\n",
    "    year_embedding = torch.cat([sin_embed, cos_embed], dim=-1)\n",
    "    if year_embedding.shape[-1] < dim: \n",
    "        pad_size = dim - year_embedding.shape[-1]\n",
    "        padding = torch.zeros(year_embedding.shape[:-1] + (pad_size,), device=year_embedding.device)\n",
    "        year_embedding = torch.cat([year_embedding, padding], dim=-1)\n",
    "    return year_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Positional Encoding (Sinusoidal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 == 1: \n",
    "            pe[:, 1::2] = torch.cos(position * div_term)[:, :pe[:, 1::2].shape[1]] \n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  \n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CompanySequenceModel: FT-Transformer + tst (Sequence 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanySequenceModel(nn.Module):\n",
    "    def __init__(self, cont_input_dim, bin_input_dim, year_dim, num_stock_embeddings, stock_dim=STOCK_DIM_PARAM, ft_out_dim=FT_OUT_DIM_PARAM, num_years=NUM_YEARS_PARAM):\n",
    "        super(CompanySequenceModel, self).__init__()\n",
    "        self.num_years = num_years\n",
    "        \n",
    "        self.cont_embedding = nn.Linear(cont_input_dim, 32)\n",
    "        self.bin_embedding = nn.Linear(bin_input_dim, 16)\n",
    "        self.stock_embedding = nn.Embedding(num_embeddings=num_stock_embeddings, embedding_dim=stock_dim)\n",
    "        \n",
    "        total_input_dim = 32 + 16 + year_dim + stock_dim\n",
    "        self.embedding = nn.Linear(total_input_dim, 128)\n",
    "        self.bn = nn.BatchNorm1d(128) \n",
    "        \n",
    "        self.ft_transformer = rtdl.FTTransformer.make_default(\n",
    "            n_num_features=128, \n",
    "            cat_cardinalities=None, \n",
    "            d_out=ft_out_dim\n",
    "        )\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=ft_out_dim, out_channels=ft_out_dim, kernel_size=3, padding=1)\n",
    "        self.pos_encoder = PositionalEncoding(ft_out_dim, max_len=num_years)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=ft_out_dim, nhead=2, dropout=0.1, batch_first=True)\n",
    "        self.tst_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "    \n",
    "    def forward(self, x_cont, x_bin, year_values, stock_id):\n",
    "        batch, num_years, _ = x_cont.shape\n",
    "        year_embed = get_sine_cosine_year_embedding(year_values, dim=YEAR_DIM_PARAM) \n",
    "        \n",
    "        cont_emb = self.cont_embedding(x_cont.reshape(-1, x_cont.shape[-1]))\n",
    "        bin_emb = self.bin_embedding(x_bin.reshape(-1, x_bin.shape[-1]))\n",
    "        \n",
    "        stock_emb = self.stock_embedding(stock_id) \n",
    "        stock_emb = stock_emb.unsqueeze(1).repeat(1, num_years, 1) \n",
    "        \n",
    "        x_all = torch.cat([\n",
    "            cont_emb, \n",
    "            bin_emb,  \n",
    "            year_embed.reshape(-1, year_embed.shape[-1]), \n",
    "            stock_emb.reshape(-1, stock_emb.shape[-1])   \n",
    "        ], dim=-1)\n",
    "        \n",
    "        x_all = self.embedding(x_all)  \n",
    "        x_all = self.bn(x_all) \n",
    "        \n",
    "        ft_out = self.ft_transformer(x_num=x_all, x_cat=None)\n",
    "        ft_out = ft_out.view(batch, num_years, -1)  \n",
    "        \n",
    "        conv_in = ft_out.transpose(1, 2)       \n",
    "        conv_out = self.conv1d(conv_in)          \n",
    "        conv_out = conv_out.transpose(1, 2)      \n",
    "        \n",
    "        tst_input = self.pos_encoder(conv_out)   \n",
    "        tst_output = self.tst_encoder(tst_input)\n",
    "        return tst_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CompanySequenceModel 학습 (FT-Transformer + Tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 모델 파라미터 및 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_input_dim = len(continuous_features)  \n",
    "bin_input_dim = len(binary_features)         \n",
    "\n",
    "if not df_filtered.empty:\n",
    "    p_big4 = (df_filtered['BIG4'] == 1).mean()\n",
    "    p_loss = (df_filtered['LOSS'] == 1).mean()\n",
    "    p_big4 = np.clip(p_big4, EPS, 1.0 - EPS)\n",
    "    p_loss = np.clip(p_loss, EPS, 1.0 - EPS)\n",
    "    pos_w  = torch.tensor([\n",
    "        ((1-p_big4)/p_big4)**0.5, \n",
    "        ((1-p_loss)/p_loss)**0.5\n",
    "    ], device=device)\n",
    "else:\n",
    "    pos_w = torch.ones(2, device=device) \n",
    "\n",
    "bce_bin   = nn.BCEWithLogitsLoss(pos_weight=pos_w)   \n",
    "mse_cont  = nn.MSELoss()                             \n",
    "λ_bin_enc = 10.0                                     \n",
    "\n",
    "if NUM_STOCK_EMBEDDINGS > 0: # Ensure there are stocks to create embeddings for\n",
    "    company_model = CompanySequenceModel(\n",
    "        cont_input_dim, bin_input_dim, YEAR_DIM_PARAM, \n",
    "        num_stock_embeddings=NUM_STOCK_EMBEDDINGS, \n",
    "        stock_dim=STOCK_DIM_PARAM, \n",
    "        ft_out_dim=FT_OUT_DIM_PARAM, \n",
    "        num_years=NUM_YEARS_PARAM\n",
    "    ).to(device)\n",
    "    optimizer_company = optim.Adam(company_model.parameters(), lr=LEARNING_RATE_COMPANY)\n",
    "    # Unique log directory for each run\n",
    "    current_time_str = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    writer_company = SummaryWriter(f'{TENSORBOARD_LOG_DIR_COMPANY}_{current_time_str}')\n",
    "else:\n",
    "    print(\"Skipping CompanySequenceModel initialization as there are no stocks after filtering.\")\n",
    "    company_model = None # Or handle as appropriate\n",
    "    optimizer_company = None\n",
    "    writer_company = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if company_model and dataloader_seq: # Check if model and dataloader are initialized\n",
    "\n",
    "    # 모델 그래프 로깅 (학습 시작 전, 첫 번째 배치 데이터 사용)\n",
    "    if writer_company: # Check if writer is initialized\n",
    "        try:\n",
    "            data_iter_company = iter(dataloader_seq) \n",
    "            sample_cont_company, sample_bin_company, sample_year_company, sample_stock_company, _ = next(data_iter_company)\n",
    "            writer_company.add_graph(company_model, [sample_cont_company.to(device), sample_bin_company.to(device), sample_year_company.to(device), sample_stock_company.to(device)])\n",
    "            del data_iter_company \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding CompanySequenceModel graph to TensorBoard: {e}\")\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS_COMPANY_MODEL):\n",
    "        epoch_loss_cont = 0.0\n",
    "        epoch_loss_bin = 0.0\n",
    "        num_batches = 0\n",
    "        for batch_cont, batch_bin, batch_year, batch_stock, batch_target in dataloader_seq:\n",
    "            batch_cont  = batch_cont.to(device)\n",
    "            batch_bin   = batch_bin.to(device)\n",
    "            batch_year  = batch_year.to(device)\n",
    "            batch_stock = batch_stock.to(device)\n",
    "            batch_target= batch_target.to(device)        \n",
    "\n",
    "            optimizer_company.zero_grad()\n",
    "            pred = company_model(batch_cont, batch_bin, batch_year, batch_stock)\n",
    "\n",
    "            pred_cont, pred_bin = pred[:, :, :cont_input_dim], pred[:, :, cont_input_dim:]\n",
    "            tgt_cont , tgt_bin  = batch_target[:, :, :cont_input_dim], batch_target[:, :, cont_input_dim:]\n",
    "\n",
    "            loss_cont = mse_cont(pred_cont, tgt_cont)\n",
    "            loss_bin  = bce_bin(pred_bin, tgt_bin)\n",
    "            loss      = loss_cont + λ_bin_enc * loss_bin\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            if writer_company and (epoch % (EPOCHS_COMPANY_MODEL // 5 if EPOCHS_COMPANY_MODEL >=5 else 1) == 0 or epoch == EPOCHS_COMPANY_MODEL -1): \n",
    "                for name, param in company_model.named_parameters():\n",
    "                    if param.grad is not None:\n",
    "                        writer_company.add_histogram(f'Gradients_Company/{name.replace(\".\", \"/\")}', param.grad, epoch)\n",
    "            \n",
    "            optimizer_company.step()\n",
    "            \n",
    "            epoch_loss_cont += loss_cont.item()\n",
    "            epoch_loss_bin += loss_bin.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss_cont = epoch_loss_cont / num_batches if num_batches > 0 else 0\n",
    "        avg_loss_bin = epoch_loss_bin / num_batches if num_batches > 0 else 0\n",
    "        total_avg_loss = avg_loss_cont + λ_bin_enc * avg_loss_bin \n",
    "\n",
    "        if writer_company:\n",
    "            writer_company.add_scalar('Loss_Company/Continuous_Train', avg_loss_cont, epoch)\n",
    "            writer_company.add_scalar('Loss_Company/Binary_Train', avg_loss_bin, epoch)\n",
    "            writer_company.add_scalar('Loss_Company/Total_Train', total_avg_loss, epoch)\n",
    "\n",
    "        if epoch % (EPOCHS_COMPANY_MODEL // 5 if EPOCHS_COMPANY_MODEL >=5 else 1) == 0 or epoch == EPOCHS_COMPANY_MODEL -1:\n",
    "            print(f\"[CompanySequenceModel] Epoch {epoch:03d} | \"\n",
    "                  f\"loss_cont={avg_loss_cont:.4f}  \"\n",
    "                  f\"loss_bin={avg_loss_bin:.4f}\")\n",
    "            \n",
    "            if writer_company:\n",
    "                for name, param in company_model.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        writer_company.add_histogram(f'Weights_Company/{name.replace(\".\", \"/\")}', param.data, epoch)\n",
    "    \n",
    "    if writer_company: writer_company.close() \n",
    "else:\n",
    "    print(\"Skipping CompanySequenceModel training as model or dataloader is not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 학습 완료 후 결과 수집 및 평탄화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = []\n",
    "all_year_outputs = [] \n",
    "all_stock_outputs = [] \n",
    "\n",
    "if company_model and dataloader_seq: # Check if model and dataloader are initialized\n",
    "    company_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_cont, batch_bin, batch_year, batch_stock, _ in dataloader_seq:\n",
    "            batch_cont = batch_cont.to(device)\n",
    "            batch_bin = batch_bin.to(device)\n",
    "            batch_year = batch_year.to(device)\n",
    "            batch_stock = batch_stock.to(device)\n",
    "            out = company_model(batch_cont, batch_bin, batch_year, batch_stock)  \n",
    "            all_outputs.append(out.cpu())\n",
    "            all_year_outputs.append(batch_year.cpu())\n",
    "            batch_stock_expanded = batch_stock.unsqueeze(1).repeat(1, NUM_YEARS_PARAM)\n",
    "            all_stock_outputs.append(batch_stock_expanded.cpu())\n",
    "\n",
    "    output_tensor_seq = torch.cat(all_outputs, dim=0)    \n",
    "    year_tensor_seq_output = torch.cat(all_year_outputs, dim=0)           \n",
    "    stock_tensor_seq_expanded = torch.cat(all_stock_outputs, dim=0) \n",
    "\n",
    "    output_tensor_flat = output_tensor_seq.reshape(-1, FT_OUT_DIM_PARAM)           \n",
    "    year_tensor_flat = year_tensor_seq_output.reshape(-1)  \n",
    "    year_embed_flat = get_sine_cosine_year_embedding(year_tensor_flat, dim=YEAR_DIM_PARAM) \n",
    "    stock_tensor_flat = stock_tensor_seq_expanded.reshape(-1)                  \n",
    "\n",
    "    print(f\"output_tensor_seq shape: {output_tensor_seq.shape}\")\n",
    "    print(f\"output_tensor_flat shape: {output_tensor_flat.shape}\")\n",
    "else:\n",
    "    print(\"Skipping CompanySequenceModel output collection as model or dataloader is not initialized.\")\n",
    "    output_tensor_seq = torch.empty(0, NUM_YEARS_PARAM, FT_OUT_DIM_PARAM) \n",
    "    year_tensor_seq_output = torch.empty(0, NUM_YEARS_PARAM)\n",
    "    stock_tensor_seq_expanded = torch.empty(0, NUM_YEARS_PARAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8'. Transformer-Denoiser 기반 Diffusion (시계열 컨텍스트 활용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Diffusion 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_diff      = 10\n",
    "beta_start  = 1e-4\n",
    "beta_end    = 2e-2\n",
    "betas       = torch.linspace(beta_start, beta_end, T_diff, device=device)      \n",
    "alphas      = 1.0 - betas\n",
    "alpha_bars  = torch.cumprod(alphas, dim=0)                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 학습/평가용 Dataset (Diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_filtered.empty and NUM_STOCK_EMBEDDINGS > 0: # Ensure original data was processed\n",
    "    stock_scalar_seq_diff = torch.tensor(stock_seq, dtype=torch.long) \n",
    "    bin_label_tensor_seq_diff = torch.tensor(X_bin_seq, dtype=torch.float32) \n",
    "else: \n",
    "    stock_scalar_seq_diff = torch.empty(0, dtype=torch.long)\n",
    "    bin_label_tensor_seq_diff = torch.empty(0, NUM_YEARS_PARAM, bin_input_dim)\n",
    "\n",
    "if output_tensor_seq.numel() > 0: # Check if output_tensor_seq is not empty\n",
    "    diff_dataset = TensorDataset(\n",
    "        output_tensor_seq,          \n",
    "        year_tensor_seq_output,     \n",
    "        stock_scalar_seq_diff,      \n",
    "        bin_label_tensor_seq_diff   \n",
    "    )\n",
    "    diff_dataloader = DataLoader(diff_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    print(f\"Diffusion dataset size: {len(diff_dataset)}\")\n",
    "else:\n",
    "    print(\"Skipping Diffusion DataLoader creation as CompanySequenceModel output is empty.\")\n",
    "    diff_dataloader = [] # Or an empty DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Sinusoidal 시간-스텝 임베딩 (`TimeEmbedding`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        half_d_model = d_model // 2\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, half_d_model, dtype=torch.float32) / half_d_model))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor: \n",
    "        sinusoidal_input = t * self.inv_freq            \n",
    "        emb = torch.cat([torch.sin(sinusoidal_input), torch.cos(sinusoidal_input)], dim=-1)  \n",
    "        if self.d_model % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Transformer-Denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDenoiser(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_stock_embeddings_denoiser, # Added parameter\n",
    "        feat_dim=FT_OUT_DIM_PARAM,         \n",
    "        d_model=DENOISER_D_MODEL,\n",
    "        nhead=4, num_layers=4,\n",
    "        stock_emb_dim=STOCK_DIM_PARAM,\n",
    "        year_pos_dim=YEAR_DIM_PARAM, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "\n",
    "        self.year_proj = nn.Linear(year_pos_dim, d_model) \n",
    "        self.stock_emb = nn.Embedding(num_stock_embeddings_denoiser, stock_emb_dim)\n",
    "        self.in_proj = nn.Linear(feat_dim + stock_emb_dim, d_model)\n",
    "\n",
    "        self.t_embed = nn.Sequential(\n",
    "            TimeEmbedding(d_model), nn.Linear(d_model, d_model), nn.SiLU()\n",
    "        )\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=NUM_YEARS_PARAM) \n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        self.out_cont = nn.Linear(d_model, cont_input_dim)   \n",
    "        self.out_bin  = nn.Linear(d_model,  bin_input_dim)   \n",
    "\n",
    "\n",
    "    def forward(self, x_t, years, stock_id, t_norm):\n",
    "        B, S, _ = x_t.shape \n",
    "        year_embed_raw = get_sine_cosine_year_embedding(\n",
    "            years, dim=self.year_proj.in_features \n",
    "        ) \n",
    "        year_embed = self.year_proj(year_embed_raw) \n",
    "\n",
    "        stock_embed_val = self.stock_emb(stock_id).unsqueeze(1).repeat(1, S, 1)\n",
    "\n",
    "        h = self.in_proj(torch.cat([x_t, stock_embed_val], dim=-1)) \n",
    "        h = self.pos_enc(h) + year_embed + self.t_embed(t_norm).unsqueeze(1) \n",
    "\n",
    "        h = self.encoder(h) \n",
    "\n",
    "        return self.out_cont(h), self.out_bin(h)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Forward Diffusion (`q_sample`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_sample(x0_seq, t_int):\n",
    "    t_idx = t_int.long() - 1 \n",
    "    sqrt_ab  = torch.sqrt(alpha_bars[t_idx]).view(-1,1,1)      \n",
    "    sqrt_1m_ab  = torch.sqrt(1-alpha_bars[t_idx]).view(-1,1,1) \n",
    "    noise    = torch.randn_like(x0_seq)\n",
    "    return sqrt_ab*x0_seq + sqrt_1m_ab*noise, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Denoiser 학습 설정 및 `snr_weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUM_STOCK_EMBEDDINGS > 0: # Ensure there are stocks to create embeddings for\n",
    "    denoiser = TransformerDenoiser(\n",
    "        num_stock_embeddings_denoiser=NUM_STOCK_EMBEDDINGS, # Pass the number of unique stocks\n",
    "        feat_dim=FT_OUT_DIM_PARAM, \n",
    "        d_model=DENOISER_D_MODEL, \n",
    "        year_pos_dim=YEAR_DIM_PARAM,\n",
    "        stock_emb_dim=STOCK_DIM_PARAM \n",
    "    ).to(device)\n",
    "    opt_denoiser = optim.AdamW(denoiser.parameters(), lr=LEARNING_RATE_DENOISER) # Renamed optimizer\n",
    "    current_time_str_denoiser = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    writer_denoiser = SummaryWriter(f'{TENSORBOARD_LOG_DIR_DENOISER}_{current_time_str_denoiser}')\n",
    "else:\n",
    "    print(\"Skipping Denoiser model initialization as there are no stocks.\")\n",
    "    denoiser = None\n",
    "    opt_denoiser = None\n",
    "    writer_denoiser = None\n",
    "\n",
    "def snr_weight(t_idx: torch.Tensor, \n",
    "               alpha_bars_local: torch.Tensor, \n",
    "               strategy: str = \"karras\",\n",
    "               rho: float = 1.2) -> torch.Tensor:\n",
    "    ab = alpha_bars_local[t_idx]                       \n",
    "    snr = ab / (1.0 - ab)\n",
    "\n",
    "    if strategy == \"karras\":                         \n",
    "        weight = (snr + 1.0).pow(-rho)\n",
    "    elif strategy == \"simple\":\n",
    "        weight = 1.0 / (snr + 1.0)\n",
    "    else:\n",
    "        raise ValueError(f\"unknown strategy {strategy}\")\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 Denoiser 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_c = nn.MSELoss(reduction='none')                   \n",
    "bce_fn      = nn.BCEWithLogitsLoss(pos_weight=pos_w,         \n",
    "                                   reduction='none')\n",
    "\n",
    "λ_bin_denoiser = 10.0   \n",
    "\n",
    "# CDF Loss 관련 가중치 (previous_version_Model.ipynb 에서 가져옴)\n",
    "γ_mean = 1.0   # 평균 매칭 항 가중치\n",
    "γ_std  = 1.0   # 분산 매칭 항 가중치\n",
    "γ_corr = 2.0  # 상관구조 매칭 항 가중치\n",
    "γ_cdf = 1.0    # CDF 손실 가중치\n",
    "\n",
    "# CDF Loss 함수 (previous_version_Model.ipynb 에서 가져옴)\n",
    "def cdf_loss(pred, target, tau=1e-2):\n",
    "    \"\"\"\n",
    "    각 feature별 누적 분포(CDF) 정렬 유사성 강제 (Soft-Sort 기반)\n",
    "    - pred, target: (B, S, D)\n",
    "    \"\"\"\n",
    "    pred_flat = pred.reshape(-1, pred.shape[-1])   # (B*S, D)\n",
    "    target_flat = target.reshape(-1, target.shape[-1])\n",
    "    \n",
    "    loss = 0.0\n",
    "    for i in range(pred_flat.shape[1]):  # feature별\n",
    "        p_sorted = soft_sort(pred_flat[:, i].unsqueeze(-1), regularization_strength=tau)\n",
    "        t_sorted = soft_sort(target_flat[:, i].unsqueeze(-1), regularization_strength=tau)\n",
    "        loss += nn.functional.mse_loss(p_sorted.to(torch.Tensor), t_sorted.to(torch.Tensor))\n",
    "    return loss\n",
    "\n",
    "\n",
    "if denoiser and diff_dataloader: # Check if model and dataloader are initialized\n",
    "\n",
    "    if writer_denoiser: # Check if writer is initialized\n",
    "        try:\n",
    "            data_iter_denoiser = iter(diff_dataloader) \n",
    "            sample_x0_diff, sample_yrs_diff, sample_st_diff, _ = next(data_iter_denoiser)\n",
    "            sample_t_norm = torch.rand(sample_x0_diff.size(0), 1, device=device) \n",
    "            writer_denoiser.add_graph(denoiser, [sample_x0_diff.to(device), sample_yrs_diff.to(device), sample_st_diff.to(device), sample_t_norm])\n",
    "            del data_iter_denoiser \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding Denoiser graph to TensorBoard: {e}\")\n",
    "\n",
    "\n",
    "    for ep in range(EPOCHS_DENOISER_MODEL):\n",
    "        epoch_loss_denoiser = 0.0\n",
    "        num_batches_denoiser = 0\n",
    "        for x0_diff, yrs_diff, st_diff, bin_true_diff in diff_dataloader:\n",
    "            x0_diff, yrs_diff, st_diff = x0_diff.to(device), yrs_diff.to(device), st_diff.to(device)\n",
    "            bin_true_diff = bin_true_diff.to(device)\n",
    "            B = x0_diff.size(0)\n",
    "\n",
    "            t_int_rand, _ = torch.sort(torch.randint(1, T_diff + 1, (B,), device=device))\n",
    "            x_t, _   = q_sample(x0_diff, t_int_rand)                 \n",
    "            t_norm   = t_int_rand.float().unsqueeze(1) / T_diff \n",
    "\n",
    "            cont_hat, bin_hat = denoiser(x_t, yrs_diff, st_diff, t_norm)   \n",
    "            bin_hat = bin_hat.clamp(-15, 15) \n",
    "\n",
    "            cont_tgt = x0_diff[:, :, :cont_input_dim] \n",
    "            bin_tgt  = bin_true_diff \n",
    "\n",
    "            # 손실 계산 (per-sample → mean)\n",
    "            mse = criterion_c(cont_hat, cont_tgt).mean(dim=(1,2))          \n",
    "            bce = bce_fn(bin_hat, bin_tgt).mean(dim=(1, 2))              \n",
    "            w   = snr_weight(t_int_rand - 1, alpha_bars, \"karras\", rho=1.2)       \n",
    "            \n",
    "            # previous_version_Model.ipynb 에서 가져온 손실 항들\n",
    "            loss_mean = (cont_hat.mean() - cont_tgt.mean()).abs()\n",
    "            loss_std  = (cont_hat.std()  - cont_tgt.std()).abs()\n",
    "            pc = cont_hat.view(-1, cont_input_dim); tc = cont_tgt.view(-1, cont_input_dim)\n",
    "            # 상관계수 계산 시 데이터가 1개 이하인 경우 NaN 발생 방지\n",
    "            if pc.size(0) > 1:\n",
    "                 loss_corr = torch.norm(torch.corrcoef(pc.T) - torch.corrcoef(tc.T), p='fro')\n",
    "            else:\n",
    "                 loss_corr = torch.zeros(1, device=device) # 데이터 부족 시 상관계수 손실 0\n",
    "            \n",
    "            loss_cdf = cdf_loss(cont_hat, cont_tgt)  \n",
    "\n",
    "            # 최종 손실 조합\n",
    "            loss = (w * mse + λ_bin_denoiser * bce).mean() \\\n",
    "                   + γ_mean*loss_mean + γ_std*loss_std + γ_corr*loss_corr \\\n",
    "                   + γ_cdf * loss_cdf\n",
    "\n",
    "            opt_denoiser.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            if writer_denoiser and (ep % (EPOCHS_DENOISER_MODEL // 5 if EPOCHS_DENOISER_MODEL >=5 else 1) == 0 or ep == EPOCHS_DENOISER_MODEL -1): \n",
    "                for name, param in denoiser.named_parameters():\n",
    "                    if param.grad is not None:\n",
    "                        writer_denoiser.add_histogram(f'Gradients_Denoiser/{name.replace(\".\", \"/\")}', param.grad, ep)\n",
    "            \n",
    "            opt_denoiser.step()\n",
    "            \n",
    "            epoch_loss_denoiser += loss.item()\n",
    "            num_batches_denoiser +=1\n",
    "\n",
    "        avg_loss_denoiser = epoch_loss_denoiser / num_batches_denoiser if num_batches_denoiser > 0 else 0\n",
    "        if writer_denoiser:\n",
    "            writer_denoiser.add_scalar('Loss_Denoiser/Total_Train', avg_loss_denoiser, ep)\n",
    "\n",
    "        if ep % (EPOCHS_DENOISER_MODEL // 5 if EPOCHS_DENOISER_MODEL >=5 else 1) == 0 or ep == EPOCHS_DENOISER_MODEL -1:\n",
    "            print(f\"[Denoiser] ep {ep:03d} | loss {avg_loss_denoiser:.5f}\")\n",
    "            \n",
    "            if writer_denoiser:\n",
    "                for name, param in denoiser.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        writer_denoiser.add_histogram(f'Weights_Denoiser/{name.replace(\".\", \"/\")}', param.data, ep)\n",
    "    \n",
    "    if writer_denoiser: writer_denoiser.close() \n",
    "else:\n",
    "    print(\"Skipping Denoiser training as model or dataloader is not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8 Reverse Diffusion Sampler (`p_sample_loop`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def p_sample_loop(model, years_vec, stock_id, seq_len=NUM_YEARS_PARAM):\n",
    "    if model is None:\n",
    "        print(\"Denoiser model is not initialized. Cannot perform sampling.\")\n",
    "        return torch.empty(seq_len, model.feat_dim if model else 16) # Return empty tensor with expected shape if possible\n",
    "\n",
    "    model.eval() \n",
    "    x = torch.randn(1, seq_len, model.feat_dim, device=device) \n",
    "    years = years_vec.unsqueeze(0).to(device)         \n",
    "    stock = torch.tensor([stock_id], device=device)   \n",
    "\n",
    "    for t_val in range(T_diff, 0, -1): \n",
    "        t_norm_sample = torch.full((1, 1), t_val / T_diff, device=device) # Normalized time step for TimeEmbedding\n",
    "        t_idx_current = t_val - 1 # 0-based index for alpha_bars, betas, alphas\n",
    "\n",
    "        # Predict noise or x0\n",
    "        cont_hat, bin_hat = model(x, years, stock, t_norm_sample)  \n",
    "        x0_hat = torch.cat([cont_hat, bin_hat], dim=-1)     \n",
    "\n",
    "        # Calculate predicted noise (epsilon_hat) from x_t and x0_hat\n",
    "        alpha_bar_t = alpha_bars[t_idx_current]\n",
    "        eps_hat = (x - alpha_bar_t.sqrt() * x0_hat) / torch.sqrt(1 - alpha_bar_t)\n",
    "\n",
    "        # Calculate mean of the reverse diffusion step (DDPM formula)\n",
    "        beta_t, alpha_t = betas[t_idx_current], alphas[t_idx_current]\n",
    "        mean = (1 / alpha_t.sqrt()) * (x - beta_t * eps_hat / torch.sqrt(1 - alpha_bar_t))\n",
    "\n",
    "        # Sample from the reverse diffusion step\n",
    "        if t_val > 1:\n",
    "            # Add noise for steps > 1\n",
    "            x = mean + beta_t.sqrt() * torch.randn_like(x)\n",
    "        else:\n",
    "            # No noise for the last step (t=1)\n",
    "            x = mean                                    \n",
    "\n",
    "    # Separate continuous and binary outputs and apply sigmoid/threshold\n",
    "    cont_final, bin_logit = x[:, :, :cont_input_dim], x[:, :, cont_input_dim:]\n",
    "    bin_prob = torch.sigmoid(bin_logit)\n",
    "    bin_final = (bin_prob > 0.5).float()\n",
    "\n",
    "    x0_final = torch.cat([cont_final, bin_final], dim=-1)   \n",
    "    return x0_final.squeeze(0)                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.9 Inverse Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(data_np: np.ndarray, scaler_cont) -> np.ndarray: \n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_np : (N, 16) ndarray\n",
    "        ├─ data_np[:,  :14]  : 연속형 14개 (logit 값)  \n",
    "        └─ data_np[:, 14:16] : 이진형 2개  (이미 0/1 - 확정)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_np : (N, 16) ndarray\n",
    "        • 연속형 : 원 단위 스케일 (Min-Max 역변환)  \n",
    "        • 이진형 : 0 / 1  (int)\n",
    "    \"\"\"\n",
    "    data_np_transformed = data_np.copy()\n",
    "\n",
    "    # ── ① 연속형 : logit → sigmoid → inverse-MinMax ──────────\n",
    "    # Ensure there are continuous features to transform\n",
    "    if cont_input_dim > 0:\n",
    "        data_np_transformed[:, :cont_input_dim] = 1.0 / (1.0 + np.exp(-data_np_transformed[:, :cont_input_dim])) # σ(logit) ∈ (0,1)\n",
    "        # Ensure the scaler is fitted and there is data to transform\n",
    "        if scaler_cont is not None and hasattr(scaler_cont, 'inverse_transform') and data_np_transformed[:, :cont_input_dim].size > 0:\n",
    "             data_np_transformed[:, :cont_input_dim] = scaler_cont.inverse_transform(data_np_transformed[:, :cont_input_dim]) # 원 스케일\n",
    "        else:\n",
    "             print(\"Warning: Continuous scaler not fitted or no continuous data to transform.\")\n",
    "\n",
    "    # ── ② 이진형 : 이미 0/1 로 확정 → 정수 캐스팅만 ────────────\n",
    "    # Ensure there are binary features to transform\n",
    "    if bin_input_dim > 0:\n",
    "        data_np_transformed[:, cont_input_dim:cont_input_dim+bin_input_dim] = data_np_transformed[:, cont_input_dim:cont_input_dim+bin_input_dim].astype(int)\n",
    "\n",
    "    return data_np_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 대량 기업 생성 & CSV 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 `generate_synthetic_companies` 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_companies(model, scaler_cont_features, num_companies_to_gen=500, \n",
    "                                 seq_len=NUM_YEARS_PARAM, start_vid=10000):\n",
    "    if model is None:\n",
    "        print(\"Denoiser model is not initialized. Skipping synthetic data generation.\")\n",
    "        # Return empty array with correct number of columns (Stock_ID, YEAR + features)\n",
    "        return np.empty((0, 2 + cont_input_dim + bin_input_dim))\n",
    "        \n",
    "    model.eval()\n",
    "    # Use NUM_STOCK_EMBEDDINGS if available, otherwise default to 1 to avoid division by zero\n",
    "    n_stock_real = NUM_STOCK_EMBEDDINGS if NUM_STOCK_EMBEDDINGS > 0 else 1\n",
    "        \n",
    "    all_rows = []\n",
    "    years_vec_gen = torch.arange(2011, 2011+seq_len, dtype=torch.float32)\n",
    "\n",
    "    for i in range(num_companies_to_gen):\n",
    "        virt_id   = start_vid + i\n",
    "        # Map virtual ID to a real stock ID for embedding lookup\n",
    "        stock_real_id = virt_id % n_stock_real \n",
    "        \n",
    "        # Generate a sequence for one synthetic company\n",
    "        x_gen     = p_sample_loop(model, years_vec_gen, stock_real_id, seq_len)\n",
    "        \n",
    "        # Ensure x_gen is not empty before inverse transform\n",
    "        if x_gen.numel() == 0:\n",
    "             print(f\"Warning: p_sample_loop returned empty tensor for virtual ID {virt_id}. Skipping.\")\n",
    "             continue\n",
    "\n",
    "        # Inverse transform the generated data\n",
    "        x_np      = inverse_transform(x_gen.cpu().numpy(), scaler_cont_features)\n",
    "        \n",
    "        # Combine generated features with virtual Stock_ID and YEAR\n",
    "        rows      = np.hstack([\n",
    "                      np.full((seq_len,1), virt_id), # Virtual Stock_ID column\n",
    "                      years_vec_gen.reshape(-1,1).numpy(), # YEAR column\n",
    "                      x_np # Generated features\n",
    "                    ])\n",
    "        all_rows.append(rows)\n",
    "\n",
    "        # Print progress\n",
    "        if (i+1) % (num_companies_to_gen // 10 if num_companies_to_gen >=10 else 1) == 0 or i == num_companies_to_gen -1:\n",
    "            print(f\"  • {i+1}/{num_companies_to_gen} synthetic companies generated\")\n",
    "\n",
    "    # Stack all generated company sequences\n",
    "    if not all_rows:\n",
    "        # Return empty array with correct number of columns if no data was generated\n",
    "        return np.empty((0, 2 + cont_input_dim + bin_input_dim))\n",
    "\n",
    "    return np.vstack(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 평가: 이진 변수 확률 히스토그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if denoiser and diff_dataloader and len(diff_dataloader) > 0: # Check if model and dataloader are initialized and not empty\n",
    "\n",
    "    denoiser.eval()\n",
    "    all_prob = []       # 확률을 쌓아둘 리스트\n",
    "    with torch.no_grad():\n",
    "        for x0_eval, yrs_eval, st_eval, _ in diff_dataloader: \n",
    "            # q(x_t | x0)  : 어떤 t 값이든 상관없지만 t=1(=가벼운 노이즈)로 해도 OK\n",
    "            t_int_eval  = torch.ones(len(x0_eval), device=device, dtype=torch.long)  \n",
    "            x_t_eval, _ = q_sample(x0_eval.to(device), t_int_eval)\n",
    "\n",
    "            # Normalized time step for the denoiser (t=1 corresponds to 1/T_diff)\n",
    "            t_norm_eval = t_int_eval.float().unsqueeze(1) / T_diff \n",
    "\n",
    "            # Get binary logit predictions from the denoiser\n",
    "            _, bin_hat_eval = denoiser(\n",
    "                x_t_eval,\n",
    "                yrs_eval.to(device),\n",
    "                st_eval.to(device),\n",
    "                t_norm_eval\n",
    "            )\n",
    "            # Apply sigmoid to get probabilities and flatten\n",
    "            prob = torch.sigmoid(bin_hat_eval).cpu().numpy().ravel()    # 1-D\n",
    "            all_prob.append(prob)\n",
    "\n",
    "    # Concatenate probabilities from all batches\n",
    "    if all_prob:\n",
    "        all_prob = np.concatenate(all_prob)          # (N_total * 2,)  두 이진 피처 합산\n",
    "        # Generate histogram\n",
    "        hist, bin_edges = np.histogram(all_prob, bins=10, range=(0.0, 1.0))\n",
    "\n",
    "        print(\"\\nProbabilities histogram for binary features (0~1):\")\n",
    "        for i in range(10):\n",
    "            print(f\"{bin_edges[i]:.1f}–{bin_edges[i+1]:.1f}: {hist[i]}\")\n",
    "    else:\n",
    "        print(\"No probabilities collected for histogram.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping probability histogram generation as Denoiser model or diff_dataloader is not initialized or empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 평가: `positive_ratio` 함수 및 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def positive_ratio(model, dataloader, threshold=0.5):\n",
    "    if model is None or not dataloader or len(dataloader) == 0:\n",
    "        print(\"Skipping positive_ratio calculation as model or dataloader is not initialized or empty.\")\n",
    "        return\n",
    "        \n",
    "    model.eval()\n",
    "    n_pred_pos = torch.zeros(bin_input_dim, device=device)   # BIG4, LOSS\n",
    "    n_true_pos = torch.zeros(bin_input_dim, device=device)\n",
    "    n_total    = 0\n",
    "\n",
    "    for x0_pr, yrs_pr, st_pr, bin_true_pr in dataloader:\n",
    "        x0_pr, yrs_pr, st_pr = x0_pr.to(device), yrs_pr.to(device), st_pr.to(device)\n",
    "        bin_true_pr = bin_true_pr.to(device)\n",
    "        B, S, _ = x0_pr.shape \n",
    "\n",
    "        # Use t=1 for a lightly noised input to the denoiser for evaluation\n",
    "        t_int_pr  = torch.ones(B, device=device, dtype=torch.long) \n",
    "        x_t_pr, _ = q_sample(x0_pr, t_int_pr)\n",
    "        t_norm_pr = t_int_pr.float().unsqueeze(1) / T_diff\n",
    "\n",
    "        # Get binary logit predictions\n",
    "        _, bin_logit_pr = model(x_t_pr, yrs_pr, st_pr, t_norm_pr)        \n",
    "        prob_pr = torch.sigmoid(bin_logit_pr)                   # Probability\n",
    "        \n",
    "        # Count positive predictions and true positives\n",
    "        pred_pos_pr = (prob_pr > threshold).sum(dim=(0, 1))     # (2,)\n",
    "        true_pos_pr = (bin_true_pr > 0.5).sum(dim=(0,1))\n",
    "        \n",
    "        n_pred_pos += pred_pos_pr\n",
    "        n_true_pos += true_pos_pr\n",
    "        n_total    += B * S\n",
    "\n",
    "    # Calculate ratios\n",
    "    ratio_pred = (n_pred_pos.cpu() / n_total).numpy() if n_total > 0 else np.zeros(bin_input_dim)\n",
    "    ratio_true = (n_true_pos.cpu() / n_total).numpy() if n_total > 0 else np.zeros(bin_input_dim)\n",
    "    diff       = ratio_pred - ratio_true\n",
    "\n",
    "    print(f\"\\n★ Positive-ratio check (thr={threshold})\")\n",
    "    for i, name in enumerate(binary_features):\n",
    "        print(f\" {name:5s}  pred={ratio_pred[i]:.3%}  \"\n",
    "              f\"true={ratio_true[i]:.3%}  diff={diff[i]:+.2%}\")\n",
    "    print()\n",
    "\n",
    "# 2) BIG4 / LOSS 양성 예측-비율 vs. 실제-비율 출력\n",
    "positive_ratio(denoiser, diff_dataloader, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 최종 데이터 생성 및 CSV 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n▶  가짜 기업 생성 시작 …\")\n",
    "\n",
    "# Ensure necessary components are initialized before generating data\n",
    "if not df_filtered.empty and minmax_scaler is not None and denoiser is not None:\n",
    "    # Generate synthetic data (using the fitted minmax_scaler)\n",
    "    fake_data = generate_synthetic_companies(denoiser, minmax_scaler, num_companies_to_gen=500) # Use num_companies_to_gen parameter\n",
    "    \n",
    "    # Check if fake data was successfully generated\n",
    "    if fake_data.shape[0] > 0: \n",
    "        # Define column names\n",
    "        raw_cols = [\"Stock_ID\", \"YEAR\"] + continuous_features + binary_features\n",
    "        # Create DataFrame\n",
    "        df_fake  = pd.DataFrame(fake_data, columns=raw_cols)\n",
    "\n",
    "        # Define the desired final column order\n",
    "        final_cols_order = [\"OWN\", \"FORN\", \"BIG4\",\"SIZE\", \"LEV\", \"CUR\", \"GRW\", \"ROA\",\n",
    "                      \"ROE\",\"CFO\", \"PPE\", \"AGE\", \"INVREC\", \"MB\", \"TQ\", \"LOSS\"]\n",
    "        # Reorder columns\n",
    "        df_fake = df_fake[[\"Stock_ID\", \"YEAR\"] + final_cols_order] \n",
    "\n",
    "        # Convert Stock_ID and YEAR to integer type\n",
    "        df_fake[\"Stock_ID\"] = df_fake[\"Stock_ID\"].astype(int)\n",
    "        df_fake[\"YEAR\"]     = df_fake[\"YEAR\"].astype(int)\n",
    "\n",
    "        # Round float columns to 8 decimal places\n",
    "        for col in final_cols_order:\n",
    "            if col in df_fake.columns:\n",
    "                 if df_fake[col].dtype == 'float64' or df_fake[col].dtype == 'float32':\n",
    "                    df_fake[col] = df_fake[col].round(8)\n",
    "\n",
    "        # Define output path and save to CSV\n",
    "        output_csv_path = \"generated_synthetic_companies.csv\"\n",
    "        df_fake.to_csv(output_csv_path, index=False)\n",
    "\n",
    "        # Print completion message and display head\n",
    "        print(f\"✅ {len(df_fake)//NUM_YEARS_PARAM}개 기업 × {NUM_YEARS_PARAM}년 시계열 저장 완료: {output_csv_path}\")\n",
    "        display(df_fake.head()) # Use display for better notebook output\n",
    "    else:\n",
    "        print(\"No fake data was generated.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping fake data generation and saving as original data was empty, scaler not fitted, or denoiser model not initialized.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dh_financial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
