# import os
# import re
# import torch
# import torch.nn as nn
# import pandas as pd
# import numpy as np
# from sklearn.preprocessing import MinMaxScaler
# from transformers import PreTrainedTokenizerFast, AutoModelForCausalLM

# # âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# # âœ… í•™ìŠµëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# model_path = "trained_llama_model"
# model = AutoModelForCausalLM.from_pretrained(model_path).to(device)
# tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)

# # âœ… í•™ìŠµëœ ì„ë² ë”© ë¶ˆëŸ¬ì˜¤ê¸°
# num_total_companies = 1284  
# hidden_dim = model.config.hidden_size  

# # âœ… Feature & Binary ì»¬ëŸ¼ ì •ì˜
# continuous_columns = ["OWN", "FORN", "SIZE", "LEV", "CUR", "GRW", "ROA", "ROE", "CFO", "PPE", "AGE", "INVREC", "MB", "TQ"]
# binary_columns = ["BIG4", "LOSS"]

# # âœ… ì„ë² ë”© ë ˆì´ì–´ ì •ì˜ ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
# company_embedding_layer = nn.Embedding(num_embeddings=num_total_companies, embedding_dim=hidden_dim).to(device)
# year_embedding_layer = nn.Embedding(num_embeddings=2023 - 2011 + 1, embedding_dim=hidden_dim).to(device)
# binary_embedding_layer = nn.Embedding(num_embeddings=2, embedding_dim=hidden_dim).to(device)
# feature_embedding_layer = nn.Linear(len(continuous_columns), hidden_dim, bias=True).to(device)

# company_embedding_layer.load_state_dict(torch.load("trained_llama_model/company_embedding.pth", map_location=device))
# year_embedding_layer.load_state_dict(torch.load("trained_llama_model/year_embedding.pth", map_location=device))
# binary_embedding_layer.load_state_dict(torch.load("trained_llama_model/binary_embedding.pth", map_location=device))
# feature_embedding_layer.load_state_dict(torch.load("trained_llama_model/feature_embedding.pth", map_location=device))

# # âœ… Dropout ì¶”ê°€
# dropout = nn.Dropout(0.1)

# # âœ… MinMaxScaler ë¡œë“œ
# data_path = "data.csv"
# df_original = pd.read_csv(data_path)
# scaler = MinMaxScaler()
# scaler.fit(df_original[continuous_columns])  

# # âœ… ìƒì„±í•  íšŒì‚¬ ë° ì—°ë„ ì„¤ì •
# num_fake_companies = 30  
# years = list(range(2011, 2024))  

# generated_data = []

# # âœ… ë°ì´í„° ìƒì„± í•¨ìˆ˜
# def generate_financial_data(company, year):
#     """
#     íŠ¹ì • íšŒì‚¬ì™€ ì—°ë„ì— ëŒ€í•œ ì¬ë¬´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
#     """

#     # âœ… ê¸°ì¡´ ë°ì´í„° ìƒ˜í”Œë§ ì—†ì´ ì…ë ¥ì„ ë¹„ì›Œë‘ 
#     input_text = f"Company: {company}, Year: {year}, " + ", ".join([f"{col}:" for col in continuous_columns + binary_columns])
#     inputs = tokenizer(input_text, return_tensors="pt").to(device)

#     # âœ… ëª¨ë¸ì´ ì§ì ‘ Feature ê°’ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„
#     generated_ids = model.generate(
#         inputs["input_ids"],
#         num_return_sequences=1,
#         do_sample=True,
#         temperature=1.0,  # ğŸ”„ ìƒ˜í”Œë§ ê· í˜• ìœ ì§€
#         top_k=50,  
#         top_p=0.95,  
#         max_new_tokens=64,  # âœ… ê¸°ì¡´ `max_length=128` ì œê±° í›„ `max_new_tokens`ë¡œ ë³€ê²½
#         pad_token_id=tokenizer.pad_token_id
#     )

#     generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
#     print(f"ğŸ” Debug - Generated Text:\n{generated_text}")  

#     # âœ… ì •ê·œ í‘œí˜„ì‹ ê°œì„  (ê³µë°± í—ˆìš©)
#     matches = re.findall(r"(\w+):\s*(-?\d*\.?\d+)", generated_text)
#     print(f"ğŸ” Debug - Extracted Matches: {matches}")  

#     if not matches:
#         print("ğŸš¨ Error: No matches found!")
#         return None, None  

#     # âœ… ëª¨ë¸ì´ ìƒì„±í•œ ë°ì´í„°ë¥¼ ìµœì¢… Feature ê°’ìœ¼ë¡œ ì‚¬ìš©
#     feature_values = {key: float(value) for key, value in matches}

#     return generated_text, feature_values


# for company_id in range(1, num_fake_companies + 1):
#     for year in years:
#         generated_text, extracted_values = generate_financial_data(company_id, year)

#         if extracted_values is None:
#             continue  

#         company_embedding = company_embedding_layer(torch.tensor(company_id % num_total_companies, dtype=torch.long, device=device)).unsqueeze(0)
#         company_embedding = dropout(company_embedding + torch.randn_like(company_embedding) * 0.2)  

#         year_embedding = year_embedding_layer(torch.tensor(year - 2011, dtype=torch.long, device=device)).unsqueeze(0)
#         year_embedding = dropout(year_embedding + torch.randn_like(year_embedding) * 0.1)

#         # âœ… ëª¨ë¸ì´ ìƒì„±í•œ Feature ê°’ ì‚¬ìš© (ê¸°ë³¸ê°’ 0.0 ì²˜ë¦¬)
#         feature_values_list = [extracted_values.get(col, 0.0) for col in continuous_columns + binary_columns]

#         combined_embedding = company_embedding + year_embedding

#         generated_data.append([company_id, year] + feature_values_list)

# # âœ… ì»¬ëŸ¼ ê°œìˆ˜ ê³ ì •í•˜ì—¬ DataFrame ìƒì„±
# expected_columns = ["Stock", "Year"] + continuous_columns + binary_columns
# df_generated = pd.DataFrame(generated_data, columns=expected_columns)

# # âœ… CSV íŒŒì¼ ì €ì¥
# df_generated.to_csv("generated_financial_data.csv", index=False)
# print("âœ… ë°ì´í„° ìƒì„± ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: generated_financial_data.csv")

# # âœ… ìƒ˜í”Œ ì¶œë ¥
# print(df_generated.head())

import os
import re
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from transformers import PreTrainedTokenizerFast, AutoModelForCausalLM

# âœ… ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# âœ… í•™ìŠµëœ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_path = "trained_llama_model"
model = AutoModelForCausalLM.from_pretrained(model_path).to(device)
tokenizer = PreTrainedTokenizerFast.from_pretrained(model_path)

# âœ… Feature & Binary ì»¬ëŸ¼ ì •ì˜
continuous_columns = ["OWN", "FORN", "SIZE", "LEV", "CUR", "GRW", "ROA", "ROE", "CFO", "PPE", "AGE", "INVREC", "MB", "TQ"]
binary_columns = ["BIG4", "LOSS"]

# âœ… ì„ë² ë”© ë ˆì´ì–´ ì •ì˜ ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
num_total_companies = 1284  
hidden_dim = model.config.hidden_size  

company_embedding_layer = nn.Embedding(num_embeddings=num_total_companies, embedding_dim=hidden_dim).to(device)
year_embedding_layer = nn.Embedding(num_embeddings=2023 - 2011 + 1, embedding_dim=hidden_dim).to(device)
binary_embedding_layer = nn.Embedding(num_embeddings=2, embedding_dim=hidden_dim).to(device)
feature_embedding_layer = nn.Linear(len(continuous_columns), hidden_dim, bias=True).to(device)

company_embedding_layer.load_state_dict(torch.load("trained_llama_model/company_embedding.pth", map_location=device, weights_only=True))
year_embedding_layer.load_state_dict(torch.load("trained_llama_model/year_embedding.pth", map_location=device, weights_only=True))
binary_embedding_layer.load_state_dict(torch.load("trained_llama_model/binary_embedding.pth", map_location=device, weights_only=True))
feature_embedding_layer.load_state_dict(torch.load("trained_llama_model/feature_embedding.pth", map_location=device, weights_only=True))

# âœ… MinMaxScaler ë¡œë“œ ë° ë°ì´í„° ì •ê·œí™”
data_path = "data.csv"
df_original = pd.read_csv(data_path)

# ğŸ”¹ ì»¬ëŸ¼ëª…ì„ ëŒ€ë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ KeyError ë°©ì§€
df_original.columns = df_original.columns.str.upper()

# ğŸ”¹ YEAR ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
if "YEAR" not in df_original.columns:
    raise KeyError("ğŸš¨ 'YEAR' ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. CSV íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”!")

# âœ… MinMaxScaler ì ìš©
scaler = MinMaxScaler()
df_original[continuous_columns] = scaler.fit_transform(df_original[continuous_columns])

# âœ… 2011ë…„ í‰ê· ê°’ ê³„ì‚° (ì´ì§„ ë³€ìˆ˜ëŠ” 0 ë˜ëŠ” 1ì´ë¯€ë¡œ ë°˜ì˜¬ë¦¼)
avg_2011 = df_original[df_original["YEAR"] == 2011][continuous_columns].mean().to_dict()
binary_avg_2011 = df_original[df_original["YEAR"] == 2011][binary_columns].mode().iloc[0].to_dict()

# âœ… ë…¸ì´ì¦ˆ ì¶”ê°€ (2011ë…„ ì´ˆê¸°ê°’ì—ë§Œ ì ìš©)
for col in continuous_columns:
    avg_2011[col] += np.random.normal(0, 0.01)

# âœ… ìƒì„±í•  íšŒì‚¬ ë° ì—°ë„ ì„¤ì •
num_fake_companies = 30  
years = list(range(2011, 2024))  
previous_data = {}  

generated_data = []

# âœ… ë°ì´í„° ìƒì„± í•¨ìˆ˜
def generate_financial_data(company, year):
    """
    íŠ¹ì • íšŒì‚¬ì™€ ì—°ë„ì— ëŒ€í•œ ì¬ë¬´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
    """

    # ğŸ”¹ 2011ë…„ ë°ì´í„°ëŠ” í‰ê· ê°’ ì‚¬ìš©
    if year == 2011:
        prev_values = avg_2011.copy()
        prev_values.update(binary_avg_2011)  
    else:
        prev_values = previous_data.get((company, year - 1), {col: 0 for col in continuous_columns + binary_columns})

    # âœ… ì„ë² ë”© ì ìš©
    company_tensor = torch.tensor([company], dtype=torch.long, device=device)
    year_tensor = torch.tensor([year - 2011], dtype=torch.long, device=device)

    company_embed = company_embedding_layer(company_tensor).squeeze(0)
    year_embed = year_embedding_layer(year_tensor).squeeze(0)

    binary_features = torch.tensor([prev_values[col] for col in binary_columns], dtype=torch.long, device=device)
    binary_embed = binary_embedding_layer(binary_features).mean(dim=0)

    continuous_features = torch.tensor([prev_values[col] for col in continuous_columns], dtype=torch.float32, device=device)
    feature_embed = feature_embedding_layer(continuous_features).squeeze(0)

    # âœ… ìµœì¢… ì„ë² ë”© ë²¡í„°
    final_embed = company_embed + year_embed + binary_embed + feature_embed

    prev_text = ", ".join([f"{col}: {prev_values[col]:.4f}" for col in continuous_columns + binary_columns])

    # âœ… ì…ë ¥ í…ìŠ¤íŠ¸
    input_text = f"This is the financial statement for this year  Company: {company}, Year: {year}, {prev_text}"
    inputs = tokenizer(input_text, return_tensors="pt").to(device)

    # âœ… ëª¨ë¸ì´ ì§ì ‘ Feature ê°’ì„ ìƒì„±í•˜ë„ë¡ ìœ ë„
    generated_ids = model.generate(
        inputs["input_ids"],
        num_return_sequences=1,
        do_sample=True,
        temperature=1.2,  
        top_k=40,  
        top_p=0.90,  
        max_new_tokens=64,  
        pad_token_id=tokenizer.pad_token_id
    )

    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    print(f"ğŸ” Debug - Generated Text:\n{generated_text}")  

    # âœ… ì •ê·œ í‘œí˜„ì‹ ê°œì„ 
    matches = re.findall(r"(\w+):\s*(-?\d*\.\d+)", generated_text)
    print(f"ğŸ” Debug - Extracted Matches: {matches}")  

    if not matches:
        print("ğŸš¨ Error: No matches found!")
        return None, None  

    # âœ… ëª¨ë¸ì´ ìƒì„±í•œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ì—¬ ë‹¤ìŒ ì—°ë„ì— ë°˜ì˜
    feature_values = {key: float(value) for key, value in matches}
    feature_values["LOSS"] = round(feature_values.get("LOSS", 0))

    previous_data[(company, year)] = feature_values
    
    return generated_text, feature_values

# âœ… ë°ì´í„° ìƒì„± ë° ì €ì¥
for company_id in range(1, num_fake_companies + 1):
    for year in years:
        generated_text, extracted_values = generate_financial_data(company_id, year)
        if extracted_values is None:
            continue  
        feature_values_list = [extracted_values.get(col, 0) for col in continuous_columns + binary_columns]
        generated_data.append([company_id, year] + feature_values_list)

df_generated = pd.DataFrame(generated_data, columns=["Stock", "Year"] + continuous_columns + binary_columns)
df_generated[continuous_columns] = scaler.inverse_transform(df_generated[continuous_columns])
df_generated.to_csv("generated_financial_data.csv", index=False)
print("âœ… ë°ì´í„° ìƒì„± ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: generated_financial_data.csv")
print(df_generated.head())
