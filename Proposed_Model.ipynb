{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Model: Company Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import rtdl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math # Added for PositionalEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 실행 환경에 따라 디바이스 결정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_csv = \"data/data2.csv\"  # 실제 파일 경로 설정\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path_csv, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(file_path_csv, encoding='euc-kr')\n",
    "\n",
    "# 불필요한 컬럼 제거 (예: 'Name' 컬럼)\n",
    "if 'Name' in df.columns:\n",
    "    df = df.drop(columns=['Name'], errors='ignore')\n",
    "\n",
    "# 2011~2023년 동안 존재하는 기업만 필터링\n",
    "stock_min_year = df.groupby(\"Stock\")[\"YEAR\"].min()\n",
    "stock_max_year = df.groupby(\"Stock\")[\"YEAR\"].max()\n",
    "\n",
    "valid_stocks_initial = stock_min_year[(stock_min_year == 2011) & (stock_max_year == 2023)].index\n",
    "df_filtered = df[df[\"Stock\"].isin(valid_stocks_initial)]\n",
    "\n",
    "# 정확히 13년치 데이터가 있는 기업만 선택\n",
    "year_counts = df_filtered.groupby(\"Stock\")[\"YEAR\"].count()\n",
    "valid_stocks_final = year_counts[year_counts == 13].index\n",
    "df_filtered = df_filtered[df_filtered[\"Stock\"].isin(valid_stocks_final)]\n",
    "df_filtered = df_filtered.sort_values(by=[\"Stock\", \"YEAR\"])\n",
    "\n",
    "print(f\"Original df shape: {df.shape}\")\n",
    "print(f\"Filtered df shape: {df_filtered.shape}\")\n",
    "print(f\"Number of unique stocks after filtering: {df_filtered['Stock'].nunique()}\")\n",
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 연속형 & 이진 변수 분리, 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [\"OWN\", \"FORN\", \"SIZE\", \"LEV\", \"CUR\", \"GRW\", \"ROA\", \"ROE\", \"CFO\", \"PPE\", \"AGE\", \"INVREC\", \"MB\", \"TQ\"]\n",
    "binary_features = [\"BIG4\", \"LOSS\"]\n",
    "\n",
    "# Stock 정보를 범주형(정수형)으로 변환\n",
    "df_filtered[\"Stock_ID\"] = df_filtered[\"Stock\"].astype('category').cat.codes\n",
    "\n",
    "# 연속형 변수 MinMax 정규화\n",
    "minmax_scaler = MinMaxScaler()\n",
    "scaled_cont = minmax_scaler.fit_transform(\n",
    "    df_filtered[continuous_features]\n",
    ")\n",
    "\n",
    "# ② logit(σ⁻¹) 변환 : [0,1] → ℝ\n",
    "EPS = 1e-6                           # 수치 안정\n",
    "scaled_cont = np.clip(scaled_cont, EPS, 1.0-EPS)\n",
    "logit_cont  = np.log(scaled_cont / (1.0 - scaled_cont))\n",
    "\n",
    "df_filtered[continuous_features] = logit_cont\n",
    "\n",
    "# 이진 변수: 0/1 정수형\n",
    "df_filtered[binary_features] = df_filtered[binary_features].astype(int)\n",
    "\n",
    "# 전체 feature 목록\n",
    "features = continuous_features + binary_features\n",
    "\n",
    "print(\"Data after normalization and transformation:\")\n",
    "df_filtered[features + ['Stock_ID']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 기업 단위 시퀀스 데이터 생성 (각 기업 13년치)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = df_filtered[\"Stock\"].unique()\n",
    "grouped_cont = []\n",
    "grouped_bin = []\n",
    "grouped_year = []\n",
    "grouped_stock = []\n",
    "\n",
    "for stock_val in stocks: # Renamed 'stock' to 'stock_val' to avoid conflict with global 'stock_seq'\n",
    "    df_stock = df_filtered[df_filtered[\"Stock\"] == stock_val].sort_values(by=\"YEAR\")\n",
    "    grouped_cont.append(df_stock[continuous_features].values)  # (13, 14)\n",
    "    grouped_bin.append(df_stock[binary_features].values)         # (13, 2)\n",
    "    grouped_year.append(df_stock[\"YEAR\"].values)                 # (13,)\n",
    "    grouped_stock.append(df_stock[\"Stock_ID\"].iloc[0])            # scalar\n",
    "\n",
    "X_cont_seq = np.stack(grouped_cont, axis=0)  # (num_stocks, 13, 14)\n",
    "X_bin_seq = np.stack(grouped_bin, axis=0)     # (num_stocks, 13, 2)\n",
    "year_seq = np.stack(grouped_year, axis=0)       # (num_stocks, 13)\n",
    "stock_seq = np.array(grouped_stock)            # (num_stocks,)\n",
    "\n",
    "# 타겟: 연속형 + 이진 (14+2=16)\n",
    "target_seq = np.concatenate([X_cont_seq, X_bin_seq], axis=-1)  # (num_stocks, 13, 16)\n",
    "\n",
    "# 텐서 변환\n",
    "X_cont_tensor = torch.tensor(X_cont_seq, dtype=torch.float32)\n",
    "X_bin_tensor = torch.tensor(X_bin_seq, dtype=torch.float32)\n",
    "year_tensor_seq_input = torch.tensor(year_seq, dtype=torch.float32) # Renamed to avoid conflict\n",
    "stock_tensor_seq_input = torch.tensor(stock_seq, dtype=torch.long) # Renamed to avoid conflict\n",
    "target_tensor_seq = torch.tensor(target_seq, dtype=torch.float32)\n",
    "\n",
    "# DataLoader 구성 (기업 단위 시퀀스)\n",
    "dataset_seq = TensorDataset(X_cont_tensor, X_bin_tensor, year_tensor_seq_input, stock_tensor_seq_input, target_tensor_seq)\n",
    "dataloader_seq = DataLoader(dataset_seq, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"X_cont_tensor shape: {X_cont_tensor.shape}\")\n",
    "print(f\"X_bin_tensor shape: {X_bin_tensor.shape}\")\n",
    "print(f\"year_tensor_seq_input shape: {year_tensor_seq_input.shape}\")\n",
    "print(f\"stock_tensor_seq_input shape: {stock_tensor_seq_input.shape}\")\n",
    "print(f\"target_tensor_seq shape: {target_tensor_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. sine-cosine 기반 연도 임베딩 함수 (sequence 지원)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sine_cosine_year_embedding(years, dim=13):\n",
    "    \"\"\"\n",
    "    - years: (batch, num_years) 또는 (num_samples,) 형태의 실제 연도값 텐서\n",
    "    - 출력: (..., dim) 형태의 연도 임베딩\n",
    "    \"\"\"\n",
    "    # 만약 입력이 1차원이면 마지막 차원에 대해 unsqueeze (num_samples,) -> (num_samples, 1)\n",
    "    # 입력이 2차원이면 (batch, num_years) -> (batch, num_years, 1)\n",
    "    if len(years.shape) < 3: # if 1D or 2D\n",
    "        years = years.unsqueeze(-1)\n",
    "        \n",
    "    half_dim = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        torch.arange(0, half_dim, dtype=torch.float32) * (-np.log(10000.0) / half_dim)\n",
    "    ).to(years.device)\n",
    "    sinusoidal_input = years * freqs  # (..., half_dim)\n",
    "    sin_embed = torch.sin(sinusoidal_input)\n",
    "    cos_embed = torch.cos(sinusoidal_input)\n",
    "    year_embedding = torch.cat([sin_embed, cos_embed], dim=-1)\n",
    "    if year_embedding.shape[-1] < dim: # Handle odd dim\n",
    "        pad_size = dim - year_embedding.shape[-1]\n",
    "        padding = torch.zeros(year_embedding.shape[:-1] + (pad_size,), device=year_embedding.device)\n",
    "        year_embedding = torch.cat([year_embedding, padding], dim=-1)\n",
    "    return year_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Positional Encoding (Sinusoidal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 == 1: # Handle odd d_model\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)[:, :pe[:, 1::2].shape[1]] \n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CompanySequenceModel: FT-Transformer + tst (Sequence 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanySequenceModel(nn.Module):\n",
    "    def __init__(self, cont_input_dim, bin_input_dim, year_dim, stock_dim=32, ft_out_dim=16, num_years=13):\n",
    "        \"\"\"\n",
    "        - cont_input_dim: 연속형 변수 개수 (예: 14)\n",
    "        - bin_input_dim: 이진 변수 개수 (예: 2)\n",
    "        - year_dim: 연도 임베딩 차원 (예: 13)\n",
    "        - stock_dim: Stock 임베딩 차원 (예: 32)\n",
    "        - ft_out_dim: FT-Transformer 최종 출력 차원 (예: 16)\n",
    "        - num_years: 시퀀스 길이 (예: 13년)\n",
    "        \"\"\"\n",
    "        super(CompanySequenceModel, self).__init__()\n",
    "        self.num_years = num_years\n",
    "        \n",
    "        # 임베딩 레이어 구성\n",
    "        self.cont_embedding = nn.Linear(cont_input_dim, 32)\n",
    "        self.bin_embedding = nn.Linear(bin_input_dim, 16)\n",
    "        num_stock_embeddings = df_filtered[\"Stock_ID\"].nunique() # Ensure df_filtered is in scope\n",
    "        self.stock_embedding = nn.Embedding(num_embeddings=num_stock_embeddings, embedding_dim=stock_dim)\n",
    "        \n",
    "        # 모든 임베딩 결합 후 차원 조정을 위한 레이어:\n",
    "        # total_input_dim = 32 (연속형) + 16 (이진) + 13 (연도) + 32 (stock) = 93\n",
    "        total_input_dim = 32 + 16 + year_dim + stock_dim\n",
    "        self.embedding = nn.Linear(total_input_dim, 128)\n",
    "        self.bn = nn.BatchNorm1d(128) # BatchNorm for (N, C) or (N, C, L)\n",
    "        \n",
    "        # rtdl 라이브러리의 FT-Transformer (행별 적용)\n",
    "        self.ft_transformer = rtdl.FTTransformer.make_default(\n",
    "            n_num_features=128, # This should match the output of self.embedding\n",
    "            cat_cardinalities=None, # No categorical features for FTTransformer itself here\n",
    "            d_out=ft_out_dim\n",
    "        )\n",
    "        \n",
    "        # ----- TST (Time Series Transformer) 부분 시작 -----\n",
    "        # 1. 1D Convolution: (batch, num_years, ft_out_dim) → (batch, num_years, ft_out_dim)\n",
    "        #    → 시계열의 로컬 패턴(단기 의존성)을 캡처\n",
    "        self.conv1d = nn.Conv1d(in_channels=ft_out_dim, out_channels=ft_out_dim, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 2. Positional Encoding: 시퀀스 순서 정보를 추가 (기존 구현 재사용)\n",
    "        self.pos_encoder = PositionalEncoding(ft_out_dim, max_len=num_years)\n",
    "        \n",
    "        # 3. TST Encoder: Transformer Encoder 블록을 2겹 쌓아 TST 역할 수행\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=ft_out_dim, nhead=2, dropout=0.1, batch_first=True)\n",
    "        self.tst_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        # ----- TST 부분 끝 -----\n",
    "    \n",
    "    def forward(self, x_cont, x_bin, year_values, stock_id):\n",
    "        batch, num_years, _ = x_cont.shape\n",
    "        \n",
    "        # 연도 임베딩: (batch, num_years, 13)\n",
    "        year_embed = get_sine_cosine_year_embedding(year_values, dim=13) # year_dim is 13\n",
    "        \n",
    "        # 연속형, 이진 변수 임베딩 (각각 (batch*num_years, 임베딩 차원))\n",
    "        # Reshape for nn.Linear: (batch, num_years, features) -> (batch*num_years, features)\n",
    "        cont_emb = self.cont_embedding(x_cont.reshape(-1, x_cont.shape[-1]))\n",
    "        bin_emb = self.bin_embedding(x_bin.reshape(-1, x_bin.shape[-1]))\n",
    "        \n",
    "        # Stock 임베딩: (batch, stock_dim) → (batch, num_years, stock_dim)\n",
    "        stock_emb = self.stock_embedding(stock_id) # (batch, stock_dim)\n",
    "        stock_emb = stock_emb.unsqueeze(1).repeat(1, num_years, 1) # (batch, num_years, stock_dim)\n",
    "        \n",
    "        # 모든 임베딩을 결합: 최종 shape → (batch*num_years, total_input_dim)\n",
    "        # Reshape year_embed and stock_emb to match cont_emb and bin_emb for concatenation\n",
    "        x_all = torch.cat([\n",
    "            cont_emb, # (batch*num_years, 32)\n",
    "            bin_emb,  # (batch*num_years, 16)\n",
    "            year_embed.reshape(-1, year_embed.shape[-1]), # (batch*num_years, 13)\n",
    "            stock_emb.reshape(-1, stock_emb.shape[-1])   # (batch*num_years, stock_dim)\n",
    "        ], dim=-1)\n",
    "        \n",
    "        x_all = self.embedding(x_all)  # (batch*num_years, 128)\n",
    "        x_all = self.bn(x_all) # BatchNorm1d expects (N, C) or (N, C, L)\n",
    "        \n",
    "        # FT-Transformer 적용: (batch*num_years, ft_out_dim)\n",
    "        # FTTransformer expects x_num of shape (N, n_features) and x_cat (optional) of shape (N, n_cat_features)\n",
    "        ft_out = self.ft_transformer(x_num=x_all, x_cat=None)\n",
    "        ft_out = ft_out.view(batch, num_years, -1)  # (batch, num_years, ft_out_dim)\n",
    "        \n",
    "        # ----- TST 파트 시작 -----\n",
    "        # 1. 1D Convolution: 입력은 (batch, num_years, ft_out_dim)\n",
    "        #    → Conv1d는 (batch, ft_out_dim, num_years) 형태를 요구하므로 전치\n",
    "        conv_in = ft_out.transpose(1, 2)       # (batch, ft_out_dim, num_years)\n",
    "        conv_out = self.conv1d(conv_in)          # (batch, ft_out_dim, num_years)\n",
    "        conv_out = conv_out.transpose(1, 2)      # (batch, num_years, ft_out_dim)\n",
    "        \n",
    "        # 2. Positional Encoding 추가\n",
    "        tst_input = self.pos_encoder(conv_out)   # (batch, num_years, ft_out_dim)\n",
    "        \n",
    "        # 3. TST Encoder 적용: (batch, num_years, ft_out_dim)\n",
    "        tst_output = self.tst_encoder(tst_input)\n",
    "        # ----- TST 파트 끝 -----\n",
    "        \n",
    "        return tst_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. CompanySequenceModel 학습 (FT-Transformer + Tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 모델 파라미터 및 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_input_dim = len(continuous_features)  # 14\n",
    "bin_input_dim = len(binary_features)         # 2\n",
    "year_dim_param = 13 # Parameter for CompanySequenceModel, actual embedding dim might vary slightly if odd\n",
    "stock_dim_param = 32 \n",
    "ft_out_dim_param = 16\n",
    "num_years_param = 13\n",
    "epochs = 5 # Reduced for quick testing, original was 200\n",
    "\n",
    "# ── 클래스 불균형 보정용 pos_weight 계산 ───────────────\n",
    "if not df_filtered.empty:\n",
    "    p_big4 = (df_filtered['BIG4'] == 1).mean()\n",
    "    p_loss = (df_filtered['LOSS'] == 1).mean()\n",
    "    \n",
    "    # Avoid division by zero if a class is not present or has 100% presence\n",
    "    p_big4 = np.clip(p_big4, EPS, 1.0 - EPS)\n",
    "    p_loss = np.clip(p_loss, EPS, 1.0 - EPS)\n",
    "\n",
    "    pos_w  = torch.tensor([                 # 클래스 불균형 보정\n",
    "        ((1-p_big4)/p_big4)**0.5,                 # √비율 완화\n",
    "        ((1-p_loss)/p_loss)**0.5\n",
    "    ], device=device)\n",
    "else:\n",
    "    pos_w = torch.ones(2, device=device) # Default if df_filtered is empty\n",
    "\n",
    "bce_bin   = nn.BCEWithLogitsLoss(pos_weight=pos_w)   # ←★\n",
    "mse_cont  = nn.MSELoss()                             # ←★\n",
    "λ_bin_enc = 10.0                                     # ←★ (10-30 탐색)\n",
    "\n",
    "company_model = CompanySequenceModel(cont_input_dim, bin_input_dim, year_dim_param, stock_dim_param, ft_out_dim_param, num_years_param).to(device)\n",
    "optimizer_company = optim.Adam(company_model.parameters(), lr=0.001)\n",
    "# criterion = nn.MSELoss() # This was defined but not directly used for combined loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_filtered.empty or len(dataloader_seq) == 0:\n",
    "    print(\"Skipping CompanySequenceModel training as there is no data.\")\n",
    "else:\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss_cont = 0.0\n",
    "        epoch_loss_bin = 0.0\n",
    "        num_batches = 0\n",
    "        for batch_cont, batch_bin, batch_year, batch_stock, batch_target in dataloader_seq:\n",
    "            batch_cont  = batch_cont.to(device)\n",
    "            batch_bin   = batch_bin.to(device)\n",
    "            batch_year  = batch_year.to(device)\n",
    "            batch_stock = batch_stock.to(device)\n",
    "            batch_target= batch_target.to(device)        # (B,13,16)\n",
    "\n",
    "            optimizer_company.zero_grad()\n",
    "            pred = company_model(batch_cont, batch_bin, batch_year, batch_stock)\n",
    "\n",
    "            # ① 타깃 분리\n",
    "            pred_cont, pred_bin = pred[:, :, :cont_input_dim], pred[:, :, cont_input_dim:]\n",
    "            tgt_cont , tgt_bin  = batch_target[:, :, :cont_input_dim], batch_target[:, :, cont_input_dim:]\n",
    "\n",
    "            # ② 손실 계산\n",
    "            loss_cont = mse_cont(pred_cont, tgt_cont)\n",
    "            loss_bin  = bce_bin(pred_bin, tgt_bin)\n",
    "            loss      = loss_cont + λ_bin_enc * loss_bin\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer_company.step()\n",
    "            \n",
    "            epoch_loss_cont += loss_cont.item()\n",
    "            epoch_loss_bin += loss_bin.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        if epoch % (epochs // 5 if epochs >=5 else 1) == 0 or epoch == epochs -1:\n",
    "            avg_loss_cont = epoch_loss_cont / num_batches if num_batches > 0 else 0\n",
    "            avg_loss_bin = epoch_loss_bin / num_batches if num_batches > 0 else 0\n",
    "            print(f\"[CompanySequenceModel] Epoch {epoch:03d} | \"\n",
    "                  f\"loss_cont={avg_loss_cont:.4f}  \"\n",
    "                  f\"loss_bin={avg_loss_bin:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 학습 완료 후 결과 수집 및 평탄화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = []\n",
    "all_year_outputs = [] # Renamed to avoid conflict\n",
    "all_stock_outputs = [] # Renamed to avoid conflict\n",
    "\n",
    "if df_filtered.empty or len(dataloader_seq) == 0:\n",
    "    print(\"Skipping CompanySequenceModel output collection as there is no data.\")\n",
    "    # Create empty tensors with expected shapes if needed downstream, or handle appropriately\n",
    "    output_tensor_seq = torch.empty(0, num_years_param, ft_out_dim_param) \n",
    "    year_tensor_seq_output = torch.empty(0, num_years_param)\n",
    "    stock_tensor_seq_expanded = torch.empty(0, num_years_param)\n",
    "else:\n",
    "    company_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_cont, batch_bin, batch_year, batch_stock, _ in dataloader_seq:\n",
    "            batch_cont = batch_cont.to(device)\n",
    "            batch_bin = batch_bin.to(device)\n",
    "            batch_year = batch_year.to(device)\n",
    "            batch_stock = batch_stock.to(device)\n",
    "            out = company_model(batch_cont, batch_bin, batch_year, batch_stock)  # (batch, 13, 16)\n",
    "            all_outputs.append(out.cpu())\n",
    "            all_year_outputs.append(batch_year.cpu())\n",
    "            # 각 기업의 stock_id를 13년치로 확장\n",
    "            batch_stock_expanded = batch_stock.unsqueeze(1).repeat(1, num_years_param)\n",
    "            all_stock_outputs.append(batch_stock_expanded.cpu())\n",
    "\n",
    "    output_tensor_seq = torch.cat(all_outputs, dim=0)    # (num_companies, 13, 16)\n",
    "    year_tensor_seq_output = torch.cat(all_year_outputs, dim=0)           # (num_companies, 13)\n",
    "    stock_tensor_seq_expanded = torch.cat(all_stock_outputs, dim=0) # (num_companies, 13)\n",
    "\n",
    "# 평탄화: 각 연도별 데이터를 diffusion 모델 학습용으로 (총 샘플수 = num_companies * 13)\n",
    "output_tensor_flat = output_tensor_seq.reshape(-1, ft_out_dim_param)           # (N, 16)\n",
    "year_tensor_flat = year_tensor_seq_output.reshape(-1)  # (num_companies*13,)\n",
    "year_embed_flat = get_sine_cosine_year_embedding(year_tensor_flat, dim=year_dim_param) # (N, 13)\n",
    "stock_tensor_flat = stock_tensor_seq_expanded.reshape(-1)                  # (N,)\n",
    "\n",
    "print(f\"output_tensor_seq shape: {output_tensor_seq.shape}\")\n",
    "print(f\"output_tensor_flat shape: {output_tensor_flat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8'. Transformer-Denoiser 기반 Diffusion (시계열 컨텍스트 활용)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Diffusion 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_diff      = 10\n",
    "beta_start  = 1e-4\n",
    "beta_end    = 2e-2\n",
    "betas       = torch.linspace(beta_start, beta_end, T_diff, device=device)      # (T,)\n",
    "alphas      = 1.0 - betas\n",
    "alpha_bars  = torch.cumprod(alphas, dim=0)                                     # (T,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 학습/평가용 Dataset (Diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ❶ 시퀀스 전체(13×16)를 한 샘플로 사용하므로 평탄화는 **제거**\n",
    "# Ensure stock_seq (original, non-tensor, from data loading) is available if df_filtered was not empty\n",
    "if not df_filtered.empty:\n",
    "    stock_scalar_seq_diff = torch.tensor(stock_seq, dtype=torch.long) # Use original stock_seq\n",
    "    bin_label_tensor_seq_diff = torch.tensor(X_bin_seq, dtype=torch.float32) # Use original X_bin_seq\n",
    "else: # Create dummy tensors if no data\n",
    "    stock_scalar_seq_diff = torch.empty(0, dtype=torch.long)\n",
    "    bin_label_tensor_seq_diff = torch.empty(0, num_years_param, bin_input_dim)\n",
    "\n",
    "diff_dataset = TensorDataset(\n",
    "    output_tensor_seq,          # (N_company, 13, 16)  — x₀ (from CompanySequenceModel)\n",
    "    year_tensor_seq_output,     # (N_company, 13)      — 실제 연도 (from CompanySequenceModel inputs)\n",
    "    stock_scalar_seq_diff,      # (N_company,)         — stock id (original)\n",
    "    bin_label_tensor_seq_diff   # (N_company, 13, 2)   — 실제 이진 레이블 (original)\n",
    ")\n",
    "diff_dataloader = DataLoader(diff_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"Diffusion dataset size: {len(diff_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Sinusoidal 시간-스텝 임베딩 (`TimeEmbedding`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\" 1-D scalar t → (d_model) sinusoidal 임베딩 \"\"\"\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        # Ensure d_model is even for sin/cos pairing\n",
    "        # If d_model is odd, this will use d_model-1 for inv_freq calculation, then pad later if needed\n",
    "        # Or, ensure d_model passed is always even.\n",
    "        # Original code implicitly handles this by arange step 2.\n",
    "        half_d_model = d_model // 2\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, half_d_model, dtype=torch.float32) / half_d_model))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor: # t: (B,1)\n",
    "        # t : (B, 1)  정규화되지 않은 정수 [1 … T]\n",
    "        sinusoid_input = t * self.inv_freq            # (B, d_model/2)\n",
    "        emb = torch.cat([torch.sin(sinusoid_input), torch.cos(sinusoid_input)], dim=-1)  # (B, d_model or d_model-1)\n",
    "        # Pad if d_model was odd\n",
    "        if self.d_model % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Transformer-Denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDenoiser(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feat_dim=16,         # 14 cont + 2 bin\n",
    "        d_model=64,\n",
    "        nhead=4, num_layers=4,\n",
    "        stock_emb_dim=32,\n",
    "        year_pos_dim=13, # Input dim for year embedding before projection\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # ────────── 공통 세팅은 그대로 ──────────\n",
    "        self.year_proj = nn.Linear(year_pos_dim, d_model) # Project year embedding to d_model\n",
    "\n",
    "        n_stock = df_filtered[\"Stock_ID\"].nunique() if not df_filtered.empty else 1 # Handle empty df\n",
    "        self.stock_emb = nn.Embedding(n_stock, stock_emb_dim)\n",
    "\n",
    "        self.in_proj = nn.Linear(feat_dim + stock_emb_dim, d_model)\n",
    "\n",
    "        self.t_embed = nn.Sequential(\n",
    "            TimeEmbedding(d_model), nn.Linear(d_model, d_model), nn.SiLU()\n",
    "        )\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=num_years_param) # max_len should be sequence length (13)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        # ── (★) 출력 헤드 두 개로 분리 ────────────────────\n",
    "        self.out_cont = nn.Linear(d_model, cont_input_dim)   # 연속형 14 (original feature count)\n",
    "        self.out_bin  = nn.Linear(d_model,  bin_input_dim)   # 이진 2 (original feature count)\n",
    "        # ────────────────────────────────────────────────\n",
    "\n",
    "    def forward(self, x_t, years, stock_id, t_norm):\n",
    "        B, S, _ = x_t.shape # Batch, Sequence_len, Features\n",
    "\n",
    "        # Year embedding: (B, S, year_pos_dim) -> (B, S, d_model)\n",
    "        # get_sine_cosine_year_embedding expects (B,S) or (B*S,) for years\n",
    "        year_embed_raw = get_sine_cosine_year_embedding(\n",
    "            years, dim=self.year_proj.in_features # year_pos_dim\n",
    "        ) # Output shape (B, S, year_pos_dim)\n",
    "        year_embed = self.year_proj(year_embed_raw) # (B, S, d_model)\n",
    "\n",
    "        # Stock embedding: (B,) -> (B, stock_emb_dim) -> (B, S, stock_emb_dim)\n",
    "        stock_embed_val = self.stock_emb(stock_id).unsqueeze(1).repeat(1, S, 1)\n",
    "\n",
    "        # Input projection: concat x_t and stock_embed_val\n",
    "        # x_t: (B, S, feat_dim), stock_embed_val: (B, S, stock_emb_dim)\n",
    "        h = self.in_proj(torch.cat([x_t, stock_embed_val], dim=-1)) # (B, S, d_model)\n",
    "        \n",
    "        # Add positional, year, and time embeddings\n",
    "        h = self.pos_enc(h) + year_embed + self.t_embed(t_norm).unsqueeze(1) # t_embed output (B, d_model)\n",
    "\n",
    "        h = self.encoder(h) # (B, S, d_model)\n",
    "\n",
    "        # ── (★) 두 개를 따로 반환 ──\n",
    "        return self.out_cont(h), self.out_bin(h)    # (B,S,14), (B,S,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Forward Diffusion (`q_sample`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward diffusion — 시퀀스 전체에 동일 t 적용\n",
    "def q_sample(x0_seq, t_int):\n",
    "    \"\"\"\n",
    "    x0_seq : (B,S,16)\n",
    "    t_int  : (B,)  1-based\n",
    "    \"\"\"\n",
    "    t_idx = t_int.long() - 1 # 0-based index for alpha_bars\n",
    "    sqrt_ab  = torch.sqrt(alpha_bars[t_idx]).view(-1,1,1)      # (B,1,1)\n",
    "    sqrt_1m_ab  = torch.sqrt(1-alpha_bars[t_idx]).view(-1,1,1) # Renamed for clarity\n",
    "    noise    = torch.randn_like(x0_seq)\n",
    "    return sqrt_ab*x0_seq + sqrt_1m_ab*noise, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6 Denoiser 학습 설정 및 `snr_weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denoiser_d_model = 64\n",
    "denoiser_feat_dim = ft_out_dim_param # Should be 16, output of CompanySequenceModel\n",
    "denoiser_year_pos_dim = year_dim_param # 13\n",
    "\n",
    "denoiser = TransformerDenoiser(\n",
    "    feat_dim=denoiser_feat_dim, \n",
    "    d_model=denoiser_d_model, \n",
    "    year_pos_dim=denoiser_year_pos_dim,\n",
    "    stock_emb_dim=stock_dim_param # Use same stock_dim as in CompanySequenceModel\n",
    ").to(device)\n",
    "opt        = optim.AdamW(denoiser.parameters(), lr=1e-3)\n",
    "\n",
    "def snr_weight(t_idx: torch.Tensor, # t_idx here is 0-based\n",
    "               alpha_bars_local: torch.Tensor, # Pass alpha_bars explicitly\n",
    "               strategy: str = \"karras\",\n",
    "               rho: float = 1.2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    • t_idx      : (B,) 0-based  정수\n",
    "    • alpha_bars_local : (T,)  torch.Tensor   – 전역 상수\n",
    "    • strategy   : {\"karras\", \"simple\"}\n",
    "        - \"karras\"  :  w = (snr + 1) ** -rho   (Karras et al., 2022 권장)\n",
    "        - \"simple\"  :  w = 1 / (snr + 1)      (Nichol&Dhariwal 후속 연구)\n",
    "    • 반환        : (B,)   loss 스칼라에 곱할 weight\n",
    "    \"\"\"\n",
    "    # SNR(t) = ᾱₜ / (1-ᾱₜ)\n",
    "    ab = alpha_bars_local[t_idx]                       # (B,)\n",
    "    snr = ab / (1.0 - ab)\n",
    "\n",
    "    if strategy == \"karras\":                         # 기본\n",
    "        weight = (snr + 1.0).pow(-rho)\n",
    "    elif strategy == \"simple\":\n",
    "        weight = 1.0 / (snr + 1.0)\n",
    "    else:\n",
    "        raise ValueError(f\"unknown strategy {strategy}\")\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7 Denoiser 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_c = nn.MSELoss(reduction='none')                   # 연속형 MSE\n",
    "bce_fn      = nn.BCEWithLogitsLoss(pos_weight=pos_w,         # 이진형 BCE (pos_w from CompanyModel part)\n",
    "                                   reduction='none')\n",
    "\n",
    "λ_bin_denoiser = 10.0   # 이진 손실 가중치 (10~40 사이에서 튜닝), renamed from λ_bin\n",
    "denoiser_epochs = 5 # Reduced for quick testing, original was 200\n",
    "\n",
    "if len(diff_dataloader) == 0:\n",
    "    print(\"Skipping Denoiser training as there is no data.\")\n",
    "else:\n",
    "    for ep in range(denoiser_epochs):\n",
    "        epoch_loss_denoiser = 0.0\n",
    "        num_batches_denoiser = 0\n",
    "        for x0_diff, yrs_diff, st_diff, bin_true_diff in diff_dataloader:\n",
    "            x0_diff, yrs_diff, st_diff = x0_diff.to(device), yrs_diff.to(device), st_diff.to(device)\n",
    "            bin_true_diff = bin_true_diff.to(device)\n",
    "            B = x0_diff.size(0)\n",
    "\n",
    "            # ① forward diffusion  q(x_t | x₀)\n",
    "            # t_int is 1-based for q_sample and snr_weight (as per original code's indexing of alpha_bars)\n",
    "            t_int_rand, _ = torch.sort(torch.randint(1, T_diff + 1, (B,), device=device))\n",
    "            x_t, _   = q_sample(x0_diff, t_int_rand)                 # (B,S,16)\n",
    "            t_norm   = t_int_rand.float().unsqueeze(1) / T_diff # (B,1)\n",
    "\n",
    "            # ② 두 헤드 예측 ──────────────── (★ 변경)\n",
    "            cont_hat, bin_hat = denoiser(x_t, yrs_diff, st_diff, t_norm)   # (B,S,14), (B,S,2)\n",
    "            bin_hat = bin_hat.clamp(-15, 15) # Clamp logits for stability\n",
    "\n",
    "            # ③ 타깃 분리\n",
    "            cont_tgt = x0_diff[:, :, :cont_input_dim] \n",
    "            bin_tgt  = bin_true_diff # This is already (B,S,2)\n",
    "\n",
    "            # ④ 손실 계산 ─────────────────── (★ 변경)\n",
    "            mse = criterion_c(cont_hat, cont_tgt).mean(dim=(1,2))          # (B,)\n",
    "            bce = bce_fn(bin_hat, bin_tgt).mean(dim=(1, 2))              # (B,)\n",
    "            # snr_weight expects 0-based t_idx for alpha_bars\n",
    "            w   = snr_weight(t_int_rand - 1, alpha_bars, \"karras\", rho=1.2)       # (B,)\n",
    "            loss = (w * mse + λ_bin_denoiser * bce).mean() \n",
    "\n",
    "            # ⑤ 옵티마이저\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            epoch_loss_denoiser += loss.item()\n",
    "            num_batches_denoiser +=1\n",
    "\n",
    "        if ep % (denoiser_epochs // 5 if denoiser_epochs >=5 else 1) == 0 or ep == denoiser_epochs -1:\n",
    "            avg_loss_denoiser = epoch_loss_denoiser / num_batches_denoiser if num_batches_denoiser > 0 else 0\n",
    "            print(f\"[Denoiser] ep {ep:03d} | loss {avg_loss_denoiser:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8 Reverse Diffusion Sampler (`p_sample_loop`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── 3. 역확산 샘플러  (x̂₀ → ε̂ 변환 후 DDPM μ) ────────────────\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, years_vec, stock_id, seq_len=13):\n",
    "    \"\"\"\n",
    "    • model     : TransformerDenoiser (연속·이진 두 헤드)\n",
    "    • years_vec : 2011…2023  1-D 텐서 (seq_len,)\n",
    "    • stock_id  : 정수 ID (원본 Stock_ID 범위)\n",
    "    반환        : (seq_len, 16)   — 14 cont(logit), 2 bin(0/1)\n",
    "    \"\"\"\n",
    "    model.eval() # Ensure model is in eval mode for sampling\n",
    "    # x_T  ~  N(0, I)\n",
    "    x = torch.randn(1, seq_len, model.feat_dim, device=device) # feat_dim from denoiser\n",
    "    years = years_vec.unsqueeze(0).to(device)         # (1,S)\n",
    "    stock = torch.tensor([stock_id], device=device)   # (1,)\n",
    "\n",
    "    for t_val in range(T_diff, 0, -1): # t_val is 1-based like in training loop t_int_rand\n",
    "        t_norm_sample = torch.full((1, 1), t_val / T_diff, device=device)\n",
    "        t_idx_current = t_val - 1 # 0-based index for alpha_bars, betas\n",
    "\n",
    "        # ① 두 헤드 출력\n",
    "        cont_hat, bin_hat = model(x, years, stock, t_norm_sample)  # (1,S,14)/(1,S,2)\n",
    "        x0_hat = torch.cat([cont_hat, bin_hat], dim=-1)     # (1,S,16)\n",
    "\n",
    "        # ② x̂₀ → ε̂  변환\n",
    "        alpha_bar_t = alpha_bars[t_idx_current]\n",
    "        eps_hat = (x - alpha_bar_t.sqrt() * x0_hat) / torch.sqrt(1 - alpha_bar_t)\n",
    "\n",
    "        # ③ DDPM μ_θ(x_t)\n",
    "        beta_t, alpha_t = betas[t_idx_current], alphas[t_idx_current]\n",
    "        mean = (1 / alpha_t.sqrt()) * (x - beta_t * eps_hat / torch.sqrt(1 - alpha_bar_t))\n",
    "\n",
    "        # ④ 샘플 / 마지막 스텝\n",
    "        if t_val > 1:\n",
    "            x = mean + beta_t.sqrt() * torch.randn_like(x)\n",
    "        else:\n",
    "            x = mean                                    # t==1 → x₀\n",
    "\n",
    "    # ── (★) 이진 로짓 → 0/1  후 반환 ───────────────────────\n",
    "    cont_final, bin_logit = x[:, :, :cont_input_dim], x[:, :, cont_input_dim:]\n",
    "    bin_prob = torch.sigmoid(bin_logit)\n",
    "    bin_final = (bin_prob > 0.5).float()\n",
    "\n",
    "    x0_final = torch.cat([cont_final, bin_final], dim=-1)   # (1,S,16)\n",
    "    return x0_final.squeeze(0)                              # (S,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.9 Inverse Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(data_np: np.ndarray, scaler_cont) -> np.ndarray: # Pass scaler\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_np : (N, 16) ndarray\n",
    "        ├─ data_np[:,  :14]  : 연속형 14개 (logit 값)  \n",
    "        └─ data_np[:, 14:16] : 이진형 2개  (이미 0/1 - 확정)\n",
    "    scaler_cont: The fitted MinMaxScaler for continuous features.\n",
    "    Returns\n",
    "    -------\n",
    "    data_np_transformed : (N, 16) ndarray\n",
    "        • 연속형 : 원 단위 스케일 (Min-Max 역변환)  \n",
    "        • 이진형 : 0 / 1  (int)\n",
    "    \"\"\"\n",
    "    data_np_transformed = data_np.copy()\n",
    "    # ── ① 연속형 : logit → sigmoid → inverse-MinMax ──────────\n",
    "    data_np_transformed[:, :cont_input_dim] = 1.0 / (1.0 + np.exp(-data_np_transformed[:, :cont_input_dim])) # σ(logit) ∈ (0,1)\n",
    "    if data_np_transformed[:, :cont_input_dim].size > 0: # Check if array is not empty\n",
    "        data_np_transformed[:, :cont_input_dim] = scaler_cont.inverse_transform(data_np_transformed[:, :cont_input_dim]) # 원 스케일\n",
    "\n",
    "    # ── ② 이진형 : 이미 0/1 로 확정 → 정수 캐스팅만 ────────────\n",
    "    data_np_transformed[:, cont_input_dim:] = data_np_transformed[:, cont_input_dim:].astype(int)\n",
    "\n",
    "    return data_np_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 대량 기업 생성 & CSV 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 `generate_synthetic_companies` 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_companies(model, num_companies_to_gen=500, # Reduced for testing\n",
    "                                 seq_len=13, start_vid=10000, scaler_cont_features):\n",
    "    model.eval()\n",
    "    n_stock_real = df_filtered[\"Stock_ID\"].nunique() if not df_filtered.empty else 1\n",
    "    if n_stock_real == 0 : n_stock_real = 1 # Avoid modulo by zero if no real stocks\n",
    "        \n",
    "    all_rows = []\n",
    "    years_vec_gen = torch.arange(2011, 2011+seq_len, dtype=torch.float32)\n",
    "\n",
    "    for i in range(num_companies_to_gen):\n",
    "        virt_id   = start_vid + i\n",
    "        stock_real_id = virt_id % n_stock_real # Use original Stock_ID range (0 to n_stock_real-1)\n",
    "        \n",
    "        x_gen     = p_sample_loop(model, years_vec_gen, stock_real_id, seq_len)\n",
    "        x_np      = inverse_transform(x_gen.cpu().numpy(), scaler_cont_features)\n",
    "        rows      = np.hstack([\n",
    "                      np.full((seq_len,1), virt_id),\n",
    "                      years_vec_gen.reshape(-1,1).numpy(), # Reshape for hstack\n",
    "                      x_np\n",
    "                    ])\n",
    "        all_rows.append(rows)\n",
    "        if (i+1) % (num_companies_to_gen // 10 if num_companies_to_gen >=10 else 1) == 0 or i == num_companies_to_gen -1:\n",
    "            print(f\"  • {i+1}/{num_companies_to_gen} synthetic companies generated\")\n",
    "\n",
    "    if not all_rows:\n",
    "        # Return an empty array with correct number of columns if no data generated\n",
    "        # Columns: Stock_ID, YEAR, 14 continuous, 2 binary = 1+1+14+2 = 18\n",
    "        return np.empty((0, 2 + cont_input_dim + bin_input_dim))\n",
    "    return np.vstack(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 평가: 이진 변수 확률 히스토그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(diff_dataloader) > 0:\n",
    "    denoiser.eval()\n",
    "    all_prob = []       # 확률을 쌓아둘 리스트\n",
    "    with torch.no_grad():\n",
    "        for x0_eval, yrs_eval, st_eval, _ in diff_dataloader: \n",
    "            x_t_eval, _ = q_sample(x0_eval.to(device),\n",
    "                              torch.ones(len(x0_eval), device=device, dtype=torch.long))  # t=1 (1-based)\n",
    "            _, bin_hat_eval = denoiser(\n",
    "                x_t_eval,\n",
    "                yrs_eval.to(device),\n",
    "                st_eval.to(device),\n",
    "                torch.ones(len(x0_eval), 1, device=device) / T_diff # t_norm = 1/T\n",
    "            )\n",
    "            prob = torch.sigmoid(bin_hat_eval).cpu().numpy().ravel()    # 1-D\n",
    "            all_prob.append(prob)\n",
    "\n",
    "    all_prob = np.concatenate(all_prob)          # (N_total * 2,)  두 이진 피처 합산\n",
    "    hist, bin_edges = np.histogram(all_prob, bins=10, range=(0.0, 1.0))\n",
    "\n",
    "    print(\"\\nProbabilities histogram for binary features (0~1):\")\n",
    "    for i in range(10):\n",
    "        print(f\"{bin_edges[i]:.1f}–{bin_edges[i+1]:.1f}: {hist[i]}\")\n",
    "else:\n",
    "    print(\"Skipping probability histogram generation as diff_dataloader is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 평가: `positive_ratio` 함수 및 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def positive_ratio(model, dataloader, threshold=0.5):\n",
    "    \"\"\"\n",
    "    학습 종료 후 BIG4·LOSS 두 플래그에 대해\n",
    "      • pred=모델 1 예측 비율\n",
    "      • true=실제 1 비율\n",
    "      • diff=차이           를 출력\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    n_pred_pos = torch.zeros(bin_input_dim, device=device)   # BIG4, LOSS (use bin_input_dim)\n",
    "    n_true_pos = torch.zeros(bin_input_dim, device=device)\n",
    "    n_total    = 0\n",
    "\n",
    "    for x0_pr, yrs_pr, st_pr, bin_true_pr in dataloader:\n",
    "        x0_pr, yrs_pr, st_pr = x0_pr.to(device), yrs_pr.to(device), st_pr.to(device)\n",
    "        bin_true_pr = bin_true_pr.to(device)\n",
    "        B, S, _ = x0_pr.shape # Batch, Sequence\n",
    "\n",
    "        # t=1 로 가벼운 노이즈 추가\n",
    "        t_int_pr  = torch.ones(B, device=device, dtype=torch.long) # 1-based\n",
    "        x_t_pr, _ = q_sample(x0_pr, t_int_pr)\n",
    "        t_norm_pr = t_int_pr.float().unsqueeze(1) / T_diff\n",
    "\n",
    "        _, bin_logit_pr = model(x_t_pr, yrs_pr, st_pr, t_norm_pr)        # (B,S,2)\n",
    "        prob_pr = torch.sigmoid(bin_logit_pr)                   # 확률\n",
    "        pred_pos_pr = (prob_pr > threshold).sum(dim=(0, 1))     # (2,)\n",
    "        true_pos_pr = (bin_true_pr > 0.5).sum(dim=(0,1))\n",
    "        \n",
    "        n_pred_pos += pred_pos_pr\n",
    "        n_true_pos += true_pos_pr\n",
    "        n_total    += B * S\n",
    "\n",
    "    ratio_pred = (n_pred_pos.cpu() / n_total).numpy() if n_total > 0 else np.zeros(bin_input_dim)\n",
    "    ratio_true = (n_true_pos.cpu() / n_total).numpy() if n_total > 0 else np.zeros(bin_input_dim)\n",
    "    diff       = ratio_pred - ratio_true\n",
    "\n",
    "    print(f\"\\n★ Positive-ratio check (thr={threshold})\")\n",
    "    for i, name in enumerate(binary_features): # Use binary_features list\n",
    "        print(f\" {name:5s}  pred={ratio_pred[i]:.3%}  \"\n",
    "              f\"true={ratio_true[i]:.3%}  diff={diff[i]:+.2%}\")\n",
    "    print()\n",
    "\n",
    "if len(diff_dataloader) > 0:\n",
    "    positive_ratio(denoiser, diff_dataloader, threshold=0.5)\n",
    "else:\n",
    "    print(\"Skipping positive_ratio calculation as diff_dataloader is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 최종 데이터 생성 및 CSV 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n▶  가짜 기업 생성 시작 …\")\n",
    "if not df_filtered.empty: # Ensure minmax_scaler is fitted\n",
    "    fake_data = generate_synthetic_companies(denoiser, num_companies_to_gen=500, scaler_cont_features=minmax_scaler)\n",
    "    # (1) 원본 순서로 먼저 DataFrame 생성  ----------------------------\n",
    "    raw_cols = [\"Stock_ID\", \"YEAR\"] + continuous_features + binary_features\n",
    "    df_fake  = pd.DataFrame(fake_data, columns=raw_cols)\n",
    "\n",
    "    # (2) 원하는 열 순서로 재배치  --------------------------------------\n",
    "    final_cols_order = [\"OWN\", \"FORN\", \"BIG4\",\"SIZE\", \"LEV\", \"CUR\", \"GRW\", \"ROA\",\n",
    "                  \"ROE\",\"CFO\", \"PPE\", \"AGE\", \"INVREC\", \"MB\", \"TQ\", \"LOSS\"]\n",
    "    df_fake = df_fake[[\"Stock_ID\", \"YEAR\"] + final_cols_order] \n",
    "\n",
    "    df_fake[\"Stock_ID\"] = df_fake[\"Stock_ID\"].astype(int)\n",
    "    df_fake[\"YEAR\"]     = df_fake[\"YEAR\"].astype(int)\n",
    "\n",
    "    for col in final_cols_order:\n",
    "        if col in df_fake.columns:\n",
    "             if df_fake[col].dtype == 'float64' or df_fake[col].dtype == 'float32':\n",
    "                df_fake[col] = df_fake[col].round(8)\n",
    "\n",
    "    output_csv_path = \"generated_synthetic_companies.csv\"\n",
    "    df_fake.to_csv(output_csv_path, index=False)\n",
    "    print(f\"✅ {len(df_fake)//num_years_param}개 기업 × {num_years_param}년 시계열 저장 완료: {output_csv_path}\")\n",
    "    print(df_fake.head())\n",
    "else:\n",
    "    print(\"Skipping fake data generation and saving as original data was empty or insufficient.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
