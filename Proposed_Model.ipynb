{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Model: Company Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd90a65f",
   "metadata": {},
   "source": [
    "## Configuration Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a920ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR_DIM_PARAM = 13\n",
    "NUM_YEARS_PARAM = 13 \n",
    "FT_OUT_DIM_PARAM = 16 \n",
    "STOCK_DIM_PARAM = 32\n",
    "DENOISER_D_MODEL = 64\n",
    "EPOCHS_COMPANY_MODEL = 5 # Reduced for quick testing, original was 200\n",
    "EPOCHS_DENOISER_MODEL = 5 # Reduced for quick testing, original was 200\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE_COMPANY = 0.001\n",
    "LEARNING_RATE_DENOISER = 1e-3\n",
    "TENSORBOARD_LOG_DIR_COMPANY = 'runs/company_model_experiment'\n",
    "TENSORBOARD_LOG_DIR_DENOISER = 'runs/denoiser_model_experiment'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88965610",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3136653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import rtdl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import math # Added for PositionalEncoding\n",
    "from torch.utils.tensorboard.writer import SummaryWriter # TensorBoard import\n",
    "import time # For unique log directory names\n",
    "#from torchsort import soft_sort # Added for CDF loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5cf079",
   "metadata": {},
   "source": [
    "## 0. 실행 환경에 따라 디바이스 결정 및 시각화 활성화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ddd5c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 509062), started 0:32:48 ago. (Use '!kill 509062' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-759377099fabf390\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-759377099fabf390\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs\n",
    "# or tensorboard --logdir runs --bind_all --port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c191c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54bda5b",
   "metadata": {},
   "source": [
    "## 1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6af6bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before preprocessing:\n",
      "Original df shape: (16692, 18)\n",
      "Number of unique stocks before filtering: 1284\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stock</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>OWN</th>\n",
       "      <th>FORN</th>\n",
       "      <th>BIG4</th>\n",
       "      <th>SIZE</th>\n",
       "      <th>LEV</th>\n",
       "      <th>CUR</th>\n",
       "      <th>GRW</th>\n",
       "      <th>ROA</th>\n",
       "      <th>ROE</th>\n",
       "      <th>CFO</th>\n",
       "      <th>PPE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>INVREC</th>\n",
       "      <th>MB</th>\n",
       "      <th>TQ</th>\n",
       "      <th>LOSS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.3286</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>1</td>\n",
       "      <td>26.532442</td>\n",
       "      <td>0.348560</td>\n",
       "      <td>1.793309</td>\n",
       "      <td>0.089570</td>\n",
       "      <td>0.053036</td>\n",
       "      <td>0.081413</td>\n",
       "      <td>0.138727</td>\n",
       "      <td>0.392632</td>\n",
       "      <td>4.744932</td>\n",
       "      <td>0.284906</td>\n",
       "      <td>0.496292</td>\n",
       "      <td>0.659244</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.3157</td>\n",
       "      <td>0.0798</td>\n",
       "      <td>1</td>\n",
       "      <td>26.550538</td>\n",
       "      <td>0.323504</td>\n",
       "      <td>1.506044</td>\n",
       "      <td>-0.047706</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.362544</td>\n",
       "      <td>4.753590</td>\n",
       "      <td>0.255722</td>\n",
       "      <td>0.743216</td>\n",
       "      <td>0.820255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.3114</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>1</td>\n",
       "      <td>26.504888</td>\n",
       "      <td>0.300014</td>\n",
       "      <td>1.467747</td>\n",
       "      <td>-0.014019</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>0.025302</td>\n",
       "      <td>0.350281</td>\n",
       "      <td>4.762174</td>\n",
       "      <td>0.262648</td>\n",
       "      <td>0.539623</td>\n",
       "      <td>0.669123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.3151</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>1</td>\n",
       "      <td>26.479532</td>\n",
       "      <td>0.281291</td>\n",
       "      <td>1.591329</td>\n",
       "      <td>-0.030732</td>\n",
       "      <td>0.015616</td>\n",
       "      <td>0.021728</td>\n",
       "      <td>0.042746</td>\n",
       "      <td>0.327404</td>\n",
       "      <td>4.770685</td>\n",
       "      <td>0.267463</td>\n",
       "      <td>0.677235</td>\n",
       "      <td>0.763162</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.3235</td>\n",
       "      <td>0.0749</td>\n",
       "      <td>1</td>\n",
       "      <td>26.469703</td>\n",
       "      <td>0.266223</td>\n",
       "      <td>1.749220</td>\n",
       "      <td>0.045576</td>\n",
       "      <td>0.017915</td>\n",
       "      <td>0.024415</td>\n",
       "      <td>0.063350</td>\n",
       "      <td>0.298817</td>\n",
       "      <td>4.779123</td>\n",
       "      <td>0.267951</td>\n",
       "      <td>0.988025</td>\n",
       "      <td>0.991312</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stock  YEAR     OWN    FORN  BIG4       SIZE       LEV       CUR       GRW  \\\n",
       "0      0  2011  0.3286  0.0256     1  26.532442  0.348560  1.793309  0.089570   \n",
       "1      0  2012  0.3157  0.0798     1  26.550538  0.323504  1.506044 -0.047706   \n",
       "2      0  2013  0.3114  0.0613     1  26.504888  0.300014  1.467747 -0.014019   \n",
       "3      0  2014  0.3151  0.0502     1  26.479532  0.281291  1.591329 -0.030732   \n",
       "4      0  2015  0.3235  0.0749     1  26.469703  0.266223  1.749220  0.045576   \n",
       "\n",
       "        ROA       ROE       CFO       PPE       AGE    INVREC        MB  \\\n",
       "0  0.053036  0.081413  0.138727  0.392632  4.744932  0.284906  0.496292   \n",
       "1  0.003863  0.005710  0.074321  0.362544  4.753590  0.255722  0.743216   \n",
       "2  0.003120  0.004458  0.025302  0.350281  4.762174  0.262648  0.539623   \n",
       "3  0.015616  0.021728  0.042746  0.327404  4.770685  0.267463  0.677235   \n",
       "4  0.017915  0.024415  0.063350  0.298817  4.779123  0.267951  0.988025   \n",
       "\n",
       "         TQ  LOSS  \n",
       "0  0.659244     0  \n",
       "1  0.820255     0  \n",
       "2  0.669123     0  \n",
       "3  0.763162     0  \n",
       "4  0.991312     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data after preprocessing (filtering):\n",
      "Filtered df shape: (16692, 18)\n",
      "Number of unique stocks after filtering: 1284\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stock</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>OWN</th>\n",
       "      <th>FORN</th>\n",
       "      <th>BIG4</th>\n",
       "      <th>SIZE</th>\n",
       "      <th>LEV</th>\n",
       "      <th>CUR</th>\n",
       "      <th>GRW</th>\n",
       "      <th>ROA</th>\n",
       "      <th>ROE</th>\n",
       "      <th>CFO</th>\n",
       "      <th>PPE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>INVREC</th>\n",
       "      <th>MB</th>\n",
       "      <th>TQ</th>\n",
       "      <th>LOSS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2011</td>\n",
       "      <td>0.3286</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>1</td>\n",
       "      <td>26.532442</td>\n",
       "      <td>0.348560</td>\n",
       "      <td>1.793309</td>\n",
       "      <td>0.089570</td>\n",
       "      <td>0.053036</td>\n",
       "      <td>0.081413</td>\n",
       "      <td>0.138727</td>\n",
       "      <td>0.392632</td>\n",
       "      <td>4.744932</td>\n",
       "      <td>0.284906</td>\n",
       "      <td>0.496292</td>\n",
       "      <td>0.659244</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>0.3157</td>\n",
       "      <td>0.0798</td>\n",
       "      <td>1</td>\n",
       "      <td>26.550538</td>\n",
       "      <td>0.323504</td>\n",
       "      <td>1.506044</td>\n",
       "      <td>-0.047706</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.362544</td>\n",
       "      <td>4.753590</td>\n",
       "      <td>0.255722</td>\n",
       "      <td>0.743216</td>\n",
       "      <td>0.820255</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.3114</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>1</td>\n",
       "      <td>26.504888</td>\n",
       "      <td>0.300014</td>\n",
       "      <td>1.467747</td>\n",
       "      <td>-0.014019</td>\n",
       "      <td>0.003120</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>0.025302</td>\n",
       "      <td>0.350281</td>\n",
       "      <td>4.762174</td>\n",
       "      <td>0.262648</td>\n",
       "      <td>0.539623</td>\n",
       "      <td>0.669123</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0.3151</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>1</td>\n",
       "      <td>26.479532</td>\n",
       "      <td>0.281291</td>\n",
       "      <td>1.591329</td>\n",
       "      <td>-0.030732</td>\n",
       "      <td>0.015616</td>\n",
       "      <td>0.021728</td>\n",
       "      <td>0.042746</td>\n",
       "      <td>0.327404</td>\n",
       "      <td>4.770685</td>\n",
       "      <td>0.267463</td>\n",
       "      <td>0.677235</td>\n",
       "      <td>0.763162</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>0.3235</td>\n",
       "      <td>0.0749</td>\n",
       "      <td>1</td>\n",
       "      <td>26.469703</td>\n",
       "      <td>0.266223</td>\n",
       "      <td>1.749220</td>\n",
       "      <td>0.045576</td>\n",
       "      <td>0.017915</td>\n",
       "      <td>0.024415</td>\n",
       "      <td>0.063350</td>\n",
       "      <td>0.298817</td>\n",
       "      <td>4.779123</td>\n",
       "      <td>0.267951</td>\n",
       "      <td>0.988025</td>\n",
       "      <td>0.991312</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stock  YEAR     OWN    FORN  BIG4       SIZE       LEV       CUR       GRW  \\\n",
       "0      0  2011  0.3286  0.0256     1  26.532442  0.348560  1.793309  0.089570   \n",
       "1      0  2012  0.3157  0.0798     1  26.550538  0.323504  1.506044 -0.047706   \n",
       "2      0  2013  0.3114  0.0613     1  26.504888  0.300014  1.467747 -0.014019   \n",
       "3      0  2014  0.3151  0.0502     1  26.479532  0.281291  1.591329 -0.030732   \n",
       "4      0  2015  0.3235  0.0749     1  26.469703  0.266223  1.749220  0.045576   \n",
       "\n",
       "        ROA       ROE       CFO       PPE       AGE    INVREC        MB  \\\n",
       "0  0.053036  0.081413  0.138727  0.392632  4.744932  0.284906  0.496292   \n",
       "1  0.003863  0.005710  0.074321  0.362544  4.753590  0.255722  0.743216   \n",
       "2  0.003120  0.004458  0.025302  0.350281  4.762174  0.262648  0.539623   \n",
       "3  0.015616  0.021728  0.042746  0.327404  4.770685  0.267463  0.677235   \n",
       "4  0.017915  0.024415  0.063350  0.298817  4.779123  0.267951  0.988025   \n",
       "\n",
       "         TQ  LOSS  \n",
       "0  0.659244     0  \n",
       "1  0.820255     0  \n",
       "2  0.669123     0  \n",
       "3  0.763162     0  \n",
       "4  0.991312     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_path_csv = \"data/Table_Data.csv\"  # 실제 파일 경로 설정\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path_csv, encoding='utf-8')\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(file_path_csv, encoding='euc-kr')\n",
    "\n",
    "# 불필요한 컬럼 제거 (예: 'Name' 컬럼)\n",
    "if 'Name' in df.columns:\n",
    "    df = df.drop(columns=['Name'], errors='ignore')\n",
    "\n",
    "# 2011~2023년 동안 존재하는 기업만 필터링\n",
    "stock_min_year = df.groupby(\"Stock\")[\"YEAR\"].min()\n",
    "stock_max_year = df.groupby(\"Stock\")[\"YEAR\"].max()\n",
    "\n",
    "valid_stocks_initial = stock_min_year[(stock_min_year == 2011) & (stock_max_year == 2023)].index\n",
    "df_filtered = df[df[\"Stock\"].isin(valid_stocks_initial)].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "if df_filtered.empty:\n",
    "    print(\"No companies found that existed continuously from 2011 to 2023. Exiting.\")\n",
    "    # Potentially exit or raise an error if no data to process\n",
    "else:\n",
    "    # 정확히 13년치 데이터가 있는 기업만 선택\n",
    "    year_counts = df_filtered.groupby(\"Stock\")[\"YEAR\"].count()\n",
    "    valid_stocks_final = year_counts[year_counts == NUM_YEARS_PARAM].index\n",
    "    df_filtered = df_filtered[df_filtered[\"Stock\"].isin(valid_stocks_final)].copy()\n",
    "    df_filtered = df_filtered.sort_values(by=[\"Stock\", \"YEAR\"])\n",
    "\n",
    "    print(\"Data before preprocessing:\")\n",
    "    print(f\"Original df shape: {df.shape}\")\n",
    "    print(f\"Number of unique stocks before filtering: {df['Stock'].nunique()}\")\n",
    "    display(df.head())\n",
    "\n",
    "    print(\"\\nData after preprocessing (filtering):\")\n",
    "    print(f\"Filtered df shape: {df_filtered.shape}\")\n",
    "    print(f\"Number of unique stocks after filtering: {df_filtered['Stock'].nunique()}\")\n",
    "    display(df_filtered.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47016fe",
   "metadata": {},
   "source": [
    "## 2. 연속형 & 이진 변수 분리, 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d17361f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after normalization and transformation:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OWN</th>\n",
       "      <th>FORN</th>\n",
       "      <th>SIZE</th>\n",
       "      <th>LEV</th>\n",
       "      <th>CUR</th>\n",
       "      <th>GRW</th>\n",
       "      <th>ROA</th>\n",
       "      <th>ROE</th>\n",
       "      <th>CFO</th>\n",
       "      <th>PPE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>INVREC</th>\n",
       "      <th>MB</th>\n",
       "      <th>TQ</th>\n",
       "      <th>BIG4</th>\n",
       "      <th>LOSS</th>\n",
       "      <th>Stock_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.714524</td>\n",
       "      <td>-3.639230</td>\n",
       "      <td>-0.332483</td>\n",
       "      <td>-0.492675</td>\n",
       "      <td>-2.954608</td>\n",
       "      <td>-0.937985</td>\n",
       "      <td>0.754126</td>\n",
       "      <td>0.930243</td>\n",
       "      <td>0.878894</td>\n",
       "      <td>0.647758</td>\n",
       "      <td>3.709221</td>\n",
       "      <td>-0.579711</td>\n",
       "      <td>-3.800607</td>\n",
       "      <td>-3.387102</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.773604</td>\n",
       "      <td>-2.445068</td>\n",
       "      <td>-0.321850</td>\n",
       "      <td>-0.620203</td>\n",
       "      <td>-3.162043</td>\n",
       "      <td>-1.194294</td>\n",
       "      <td>0.469930</td>\n",
       "      <td>0.719498</td>\n",
       "      <td>0.394065</td>\n",
       "      <td>0.431259</td>\n",
       "      <td>3.802627</td>\n",
       "      <td>-0.744595</td>\n",
       "      <td>-3.148134</td>\n",
       "      <td>-2.870179</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.793582</td>\n",
       "      <td>-2.728716</td>\n",
       "      <td>-0.348709</td>\n",
       "      <td>-0.744359</td>\n",
       "      <td>-3.192924</td>\n",
       "      <td>-1.128321</td>\n",
       "      <td>0.465802</td>\n",
       "      <td>0.716146</td>\n",
       "      <td>0.056219</td>\n",
       "      <td>0.346058</td>\n",
       "      <td>3.904280</td>\n",
       "      <td>-0.704655</td>\n",
       "      <td>-3.653171</td>\n",
       "      <td>-3.347222</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.776383</td>\n",
       "      <td>-2.940236</td>\n",
       "      <td>-0.363681</td>\n",
       "      <td>-0.847365</td>\n",
       "      <td>-3.096265</td>\n",
       "      <td>-1.160768</td>\n",
       "      <td>0.535794</td>\n",
       "      <td>0.762696</td>\n",
       "      <td>0.175092</td>\n",
       "      <td>0.190144</td>\n",
       "      <td>4.015930</td>\n",
       "      <td>-0.677205</td>\n",
       "      <td>-3.287325</td>\n",
       "      <td>-3.027016</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.737733</td>\n",
       "      <td>-2.513748</td>\n",
       "      <td>-0.369496</td>\n",
       "      <td>-0.933433</td>\n",
       "      <td>-2.984015</td>\n",
       "      <td>-1.016739</td>\n",
       "      <td>0.548808</td>\n",
       "      <td>0.770009</td>\n",
       "      <td>0.317183</td>\n",
       "      <td>-0.001703</td>\n",
       "      <td>4.139951</td>\n",
       "      <td>-0.674435</td>\n",
       "      <td>-2.746297</td>\n",
       "      <td>-2.499389</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        OWN      FORN      SIZE       LEV       CUR       GRW       ROA  \\\n",
       "0 -0.714524 -3.639230 -0.332483 -0.492675 -2.954608 -0.937985  0.754126   \n",
       "1 -0.773604 -2.445068 -0.321850 -0.620203 -3.162043 -1.194294  0.469930   \n",
       "2 -0.793582 -2.728716 -0.348709 -0.744359 -3.192924 -1.128321  0.465802   \n",
       "3 -0.776383 -2.940236 -0.363681 -0.847365 -3.096265 -1.160768  0.535794   \n",
       "4 -0.737733 -2.513748 -0.369496 -0.933433 -2.984015 -1.016739  0.548808   \n",
       "\n",
       "        ROE       CFO       PPE       AGE    INVREC        MB        TQ  BIG4  \\\n",
       "0  0.930243  0.878894  0.647758  3.709221 -0.579711 -3.800607 -3.387102     1   \n",
       "1  0.719498  0.394065  0.431259  3.802627 -0.744595 -3.148134 -2.870179     1   \n",
       "2  0.716146  0.056219  0.346058  3.904280 -0.704655 -3.653171 -3.347222     1   \n",
       "3  0.762696  0.175092  0.190144  4.015930 -0.677205 -3.287325 -3.027016     1   \n",
       "4  0.770009  0.317183 -0.001703  4.139951 -0.674435 -2.746297 -2.499389     1   \n",
       "\n",
       "   LOSS  Stock_ID  \n",
       "0     0         0  \n",
       "1     0         0  \n",
       "2     0         0  \n",
       "3     0         0  \n",
       "4     0         0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "continuous_features = [\"OWN\", \"FORN\", \"SIZE\", \"LEV\", \"CUR\", \"GRW\", \"ROA\", \"ROE\", \"CFO\", \"PPE\", \"AGE\", \"INVREC\", \"MB\", \"TQ\"]\n",
    "binary_features = [\"BIG4\", \"LOSS\"]\n",
    "\n",
    "if not df_filtered.empty:\n",
    "    # Stock 정보를 범주형(정수형)으로 변환\n",
    "    df_filtered.loc[:, \"Stock_ID\"] = df_filtered[\"Stock\"].astype('category').cat.codes\n",
    "\n",
    "    # 연속형 변수 MinMax 정규화\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    scaled_cont = minmax_scaler.fit_transform(\n",
    "        df_filtered[continuous_features]\n",
    "    )\n",
    "\n",
    "    # ② logit(σ⁻¹) 변환 : [0,1] → ℝ\n",
    "    EPS = 1e-6                           # 수치 안정\n",
    "    scaled_cont = np.clip(scaled_cont, EPS, 1.0-EPS)\n",
    "    logit_cont  = np.log(scaled_cont / (1.0 - scaled_cont))\n",
    "\n",
    "    df_filtered.loc[:, continuous_features] = logit_cont\n",
    "\n",
    "    # 이진 변수: 0/1 정수형\n",
    "    df_filtered.loc[:, binary_features] = df_filtered[binary_features].astype(int)\n",
    "\n",
    "    # 전체 feature 목록\n",
    "    features = continuous_features + binary_features\n",
    "\n",
    "    print(\"Data after normalization and transformation:\")\n",
    "    display(df_filtered[features + ['Stock_ID']].head()) # Use display for better notebook output\n",
    "else:\n",
    "    print(\"Skipping normalization and transformation as df_filtered is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82d00e",
   "metadata": {},
   "source": [
    "## 3. 기업 단위 시퀀스 데이터 생성 (각 기업 13년치)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9a0b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_cont_tensor shape: torch.Size([1284, 13, 14])\n",
      "X_bin_tensor shape: torch.Size([1284, 13, 2])\n",
      "year_tensor_seq shape: torch.Size([1284, 13])\n",
      "stock_tensor_seq shape: torch.Size([1284])\n",
      "target_tensor_seq shape: torch.Size([1284, 13, 16])\n"
     ]
    }
   ],
   "source": [
    "if not df_filtered.empty:\n",
    "    stocks = df_filtered[\"Stock\"].unique()\n",
    "    grouped_cont = []\n",
    "    grouped_bin = []\n",
    "    grouped_year = []\n",
    "    grouped_stock = []\n",
    "\n",
    "    for stock_val in stocks: \n",
    "        df_stock = df_filtered[df_filtered[\"Stock\"] == stock_val].sort_values(by=\"YEAR\")\n",
    "        grouped_cont.append(df_stock[continuous_features].values)  \n",
    "        grouped_bin.append(df_stock[binary_features].values)         \n",
    "        grouped_year.append(df_stock[\"YEAR\"].values)                 \n",
    "        grouped_stock.append(df_stock[\"Stock_ID\"].iloc[0])            \n",
    "\n",
    "    # 각 주식에 대해 연속형 및 이진 특성을 시퀀스 형태로 변환\n",
    "    # grouped_cont: (num_years, num_continuous_features)\n",
    "    # grouped_bin: (num_years, num_binary_features)\n",
    "    # grouped_year: (num_years,)\n",
    "    # grouped_stock: (num_years,)\n",
    "        \n",
    "    X_cont_seq = np.stack(grouped_cont, axis=0)  \n",
    "    X_bin_seq = np.stack(grouped_bin, axis=0)     \n",
    "    year_seq = np.stack(grouped_year, axis=0)       \n",
    "    stock_seq = np.array(grouped_stock)            \n",
    "\n",
    "    # X_cont_seq: (num_stocks, num_years, num_continuous_features)\n",
    "    # X_bin_seq: (num_stocks, num_years, num_binary_features)\n",
    "    # year_seq: (num_stocks, num_years)\n",
    "    # stock_seq: (num_stocks,)\n",
    "\n",
    "\n",
    "    target_seq = np.concatenate([X_cont_seq, X_bin_seq], axis=-1)  \n",
    "\n",
    "    X_cont_tensor = torch.tensor(X_cont_seq, dtype=torch.float32)\n",
    "    X_bin_tensor = torch.tensor(X_bin_seq, dtype=torch.float32)\n",
    "    year_tensor_seq = torch.tensor(year_seq, dtype=torch.float32) \n",
    "    stock_tensor_seq = torch.tensor(stock_seq, dtype=torch.long) \n",
    "    target_tensor_seq = torch.tensor(target_seq, dtype=torch.float32)\n",
    "\n",
    "    # 시퀀스 데이터셋 생성\n",
    "    # X_cont_tensor: (num_stocks, num_years, num_continuous_features)\n",
    "    # X_bin_tensor: (num_stocks, num_years, num_binary_features)\n",
    "    # year_tensor_seq: (num_stocks, num_years)\n",
    "    # stock_tensor_seq: (num_stocks,)\n",
    "    # target_tensor_seq: (num_stocks, num_years, num_continuous_features + num_binary_features)\n",
    "\n",
    "\n",
    "    dataset_seq = TensorDataset(X_cont_tensor, X_bin_tensor, year_tensor_seq, stock_tensor_seq, target_tensor_seq)\n",
    "    dataloader_seq = DataLoader(dataset_seq, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    print(f\"X_cont_tensor shape: {X_cont_tensor.shape}\")\n",
    "    print(f\"X_bin_tensor shape: {X_bin_tensor.shape}\")\n",
    "    print(f\"year_tensor_seq shape: {year_tensor_seq.shape}\")\n",
    "    print(f\"stock_tensor_seq shape: {stock_tensor_seq.shape}\")\n",
    "    print(f\"target_tensor_seq shape: {target_tensor_seq.shape}\")\n",
    "    NUM_STOCK_EMBEDDINGS = df_filtered[\"Stock_ID\"].nunique()\n",
    "else:\n",
    "    print(\"Skipping sequence data generation as df_filtered is empty.\")\n",
    "    # Initialize with empty tensors or handle appropriately if needed downstream\n",
    "    X_cont_tensor = torch.empty(0)\n",
    "    X_bin_tensor = torch.empty(0)\n",
    "    year_tensor_seq = torch.empty(0)\n",
    "    stock_tensor_seq = torch.empty(0)\n",
    "    target_tensor_seq = torch.empty(0)\n",
    "    dataloader_seq = [] # Or an empty DataLoader\n",
    "    NUM_STOCK_EMBEDDINGS = 0 # Default if no stocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de0024",
   "metadata": {},
   "source": [
    "## 4. sine-cosine 기반 연도 임베딩 함수 (sequence 지원)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a4de521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sine_cosine_year_embedding(years, dim=YEAR_DIM_PARAM):\n",
    "    \"\"\"\n",
    "    - years: (batch, num_years) 또는 (num_samples,) 형태의 실제 연도값 텐서\n",
    "    - 출력: (..., dim) 형태의 연도 임베딩\n",
    "    \"\"\"\n",
    "    if len(years.shape) < 3: \n",
    "        years = years.unsqueeze(-1) #\n",
    "        \n",
    "    half_dim = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        torch.arange(0, half_dim, dtype=torch.float32) * (-np.log(10000.0) / half_dim)\n",
    "    ).to(years.device)\n",
    "    sinusoidal_input = years * freqs  \n",
    "    sin_embed = torch.sin(sinusoidal_input)\n",
    "    cos_embed = torch.cos(sinusoidal_input)\n",
    "    year_embedding = torch.cat([sin_embed, cos_embed], dim=-1)\n",
    "    if year_embedding.shape[-1] < dim: \n",
    "        pad_size = dim - year_embedding.shape[-1]\n",
    "        padding = torch.zeros(year_embedding.shape[:-1] + (pad_size,), device=year_embedding.device)\n",
    "        year_embedding = torch.cat([year_embedding, padding], dim=-1)\n",
    "\n",
    "    \n",
    "    return year_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f633d5d0",
   "metadata": {},
   "source": [
    "## 5. Positional Encoding (Sinusoidal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e0f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        if d_model % 2 == 1: \n",
    "            pe[:, 1::2] = torch.cos(position * div_term)[:, :pe[:, 1::2].shape[1]] \n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  \n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca17d33c",
   "metadata": {},
   "source": [
    "## 6. CompanySequenceModel: FT-Transformer + tst (Sequence 모델)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadae78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompanySequenceModel(nn.Module):\n",
    "    def __init__(self, cont_input_dim, bin_input_dim, year_dim, num_stock_embeddings, stock_dim=STOCK_DIM_PARAM, ft_out_dim=FT_OUT_DIM_PARAM, num_years=NUM_YEARS_PARAM):\n",
    "        super(CompanySequenceModel, self).__init__()\n",
    "        self.num_years = num_years\n",
    "        \n",
    "        self.cont_embedding = nn.Linear(cont_input_dim, 32)\n",
    "        self.bin_embedding = nn.Linear(bin_input_dim, 16)\n",
    "        self.stock_embedding = nn.Embedding(num_embeddings=num_stock_embeddings, embedding_dim=stock_dim)\n",
    "        \n",
    "        total_input_dim = 32 + 16 + year_dim + stock_dim\n",
    "        self.embedding = nn.Linear(total_input_dim, 128)\n",
    "        self.bn = nn.BatchNorm1d(128) \n",
    "        \n",
    "        self.ft_transformer = rtdl.FTTransformer.make_default(\n",
    "            n_num_features=128, \n",
    "            cat_cardinalities=None, \n",
    "            d_out=ft_out_dim\n",
    "        )\n",
    "        \n",
    "        self.conv1d = nn.Conv1d(in_channels=ft_out_dim, out_channels=ft_out_dim, kernel_size=3, padding=1)\n",
    "        self.pos_encoder = PositionalEncoding(ft_out_dim, max_len=num_years)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=ft_out_dim, nhead=2, dropout=0.1, batch_first=True)\n",
    "        self.tst_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "    \n",
    "    def forward(self, x_cont, x_bin, x_year_values, x_stock_id):\n",
    "        batch, num_years, _ = x_cont.shape\n",
    "        year_embed = get_sine_cosine_year_embedding(x_year_values, dim=YEAR_DIM_PARAM) \n",
    "        \n",
    "        cont_emb = self.cont_embedding(x_cont.reshape(-1, x_cont.shape[-1]))\n",
    "        bin_emb = self.bin_embedding(x_bin.reshape(-1, x_bin.shape[-1]))\n",
    "        \n",
    "        stock_emb = self.stock_embedding(x_stock_id) \n",
    "        stock_emb = stock_emb.unsqueeze(1).repeat(1, num_years, 1) \n",
    "        \n",
    "        x_all = torch.cat([\n",
    "            cont_emb, \n",
    "            bin_emb,  \n",
    "            year_embed.reshape(-1, year_embed.shape[-1]), \n",
    "            stock_emb.reshape(-1, stock_emb.shape[-1])   \n",
    "        ], dim=-1)\n",
    "        \n",
    "        x_all = self.embedding(x_all)  \n",
    "        x_all = self.bn(x_all) \n",
    "        \n",
    "        ft_out = self.ft_transformer(x_num=x_all, x_cat=None)\n",
    "        ft_out = ft_out.view(batch, num_years, -1)  \n",
    "        \n",
    "        conv_in = ft_out.transpose(1, 2)       \n",
    "        conv_out = self.conv1d(conv_in)          \n",
    "        conv_out = conv_out.transpose(1, 2)      \n",
    "        \n",
    "        tst_input = self.pos_encoder(conv_out)   \n",
    "        tst_output = self.tst_encoder(tst_input)\n",
    "        return tst_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1a6356",
   "metadata": {},
   "source": [
    "## 7. CompanySequenceModel 학습 (FT-Transformer + Tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a544cfb",
   "metadata": {},
   "source": [
    "### 7.1 모델 파라미터 및 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4186e750",
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_input_dim = len(continuous_features)  \n",
    "bin_input_dim = len(binary_features)         \n",
    "\n",
    "if not df_filtered.empty:\n",
    "    p_big4 = (df_filtered['BIG4'] == 1).mean()\n",
    "    p_loss = (df_filtered['LOSS'] == 1).mean()\n",
    "    p_big4 = np.clip(p_big4, EPS, 1.0 - EPS)\n",
    "    p_loss = np.clip(p_loss, EPS, 1.0 - EPS)\n",
    "    pos_w  = torch.tensor([\n",
    "        ((1-p_big4)/p_big4)**0.5, \n",
    "        ((1-p_loss)/p_loss)**0.5\n",
    "    ], device=device)\n",
    "else:\n",
    "    pos_w = torch.ones(2, device=device) \n",
    "\n",
    "bce_bin   = nn.BCEWithLogitsLoss(pos_weight=pos_w)   \n",
    "mse_cont  = nn.MSELoss()                             \n",
    "λ_bin_enc = 10.0                                     \n",
    "\n",
    "if NUM_STOCK_EMBEDDINGS > 0: # Ensure there are stocks to create embeddings for\n",
    "    company_model = CompanySequenceModel(\n",
    "        cont_input_dim, bin_input_dim, YEAR_DIM_PARAM, \n",
    "        num_stock_embeddings=NUM_STOCK_EMBEDDINGS, \n",
    "        stock_dim=STOCK_DIM_PARAM, \n",
    "        ft_out_dim=FT_OUT_DIM_PARAM, \n",
    "        num_years=NUM_YEARS_PARAM\n",
    "    ).to(device)\n",
    "    optimizer_company = optim.Adam(company_model.parameters(), lr=LEARNING_RATE_COMPANY)\n",
    "    # Unique log directory for each run\n",
    "    current_time_str = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    writer_company = SummaryWriter(f'{TENSORBOARD_LOG_DIR_COMPANY}_{current_time_str}')\n",
    "else:\n",
    "    print(\"Skipping CompanySequenceModel initialization as there are no stocks after filtering.\")\n",
    "    company_model = None # Or handle as appropriate\n",
    "    optimizer_company = None\n",
    "    writer_company = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042b16b1",
   "metadata": {},
   "source": [
    "### 7.2 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74443e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if company_model and dataloader_seq: # Check if model and dataloader are initialized\n",
    "    # 모델 그래프 로깅 (학습 시작 전, 첫 번째 배치 데이터 사용)\n",
    "    if writer_company: # Check if writer is initialized\n",
    "        try:\n",
    "            data_iter_company = iter(dataloader_seq) \n",
    "            sample_cont_company, sample_bin_company, sample_year_company, sample_stock_company, _ = next(data_iter_company)\n",
    "            writer_company.add_graph(company_model, [sample_cont_company.to(device), sample_bin_company.to(device), sample_year_company.to(device), sample_stock_company.to(device)])\n",
    "            del data_iter_company \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding CompanySequenceModel graph to TensorBoard: {e}\")\n",
    "\n",
    "    for epoch in range(EPOCHS_COMPANY_MODEL):\n",
    "        epoch_loss_cont = 0.0\n",
    "        epoch_loss_bin = 0.0\n",
    "        num_batches = 0\n",
    "        for batch_cont, batch_bin, batch_year, batch_stock, batch_target in dataloader_seq:\n",
    "            batch_cont  = batch_cont.to(device)\n",
    "            batch_bin   = batch_bin.to(device)\n",
    "            batch_year  = batch_year.to(device)\n",
    "            batch_stock = batch_stock.to(device)\n",
    "            batch_target= batch_target.to(device)        \n",
    "\n",
    "            optimizer_company.zero_grad()\n",
    "            pred = company_model(batch_cont, batch_bin, batch_year, batch_stock)\n",
    "\n",
    "            pred_cont, pred_bin = pred[:, :, :cont_input_dim], pred[:, :, cont_input_dim:]\n",
    "            tgt_cont , tgt_bin  = batch_target[:, :, :cont_input_dim], batch_target[:, :, cont_input_dim:]\n",
    "\n",
    "            loss_cont = mse_cont(pred_cont, tgt_cont)\n",
    "            loss_bin  = bce_bin(pred_bin, tgt_bin)\n",
    "            loss      = loss_cont + λ_bin_enc * loss_bin\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            if writer_company and (epoch % (EPOCHS_COMPANY_MODEL // 5 if EPOCHS_COMPANY_MODEL >=5 else 1) == 0 or epoch == EPOCHS_COMPANY_MODEL -1): \n",
    "                for name, param in company_model.named_parameters():\n",
    "                    if param.grad is not None:\n",
    "                        writer_company.add_histogram(f'Gradients_Company/{name.replace(\".\", \"/\")}', param.grad, epoch)\n",
    "            \n",
    "            if optimizer_company:  # Ensure optimizer is initialized\n",
    "                optimizer_company.step()\n",
    "            \n",
    "            epoch_loss_cont += loss_cont.item()\n",
    "            epoch_loss_bin += loss_bin.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss_cont = epoch_loss_cont / num_batches if num_batches > 0 else 0\n",
    "        avg_loss_bin = epoch_loss_bin / num_batches if num_batches > 0 else 0\n",
    "        total_avg_loss = avg_loss_cont + λ_bin_enc * avg_loss_bin \n",
    "\n",
    "        if writer_company:\n",
    "            writer_company.add_scalar('Loss_Company/Continuous_Train', avg_loss_cont, epoch)\n",
    "            writer_company.add_scalar('Loss_Company/Binary_Train', avg_loss_bin, epoch)\n",
    "            writer_company.add_scalar('Loss_Company/Total_Train', total_avg_loss, epoch)\n",
    "\n",
    "        if epoch % (EPOCHS_COMPANY_MODEL // 5 if EPOCHS_COMPANY_MODEL >=5 else 1) == 0 or epoch == EPOCHS_COMPANY_MODEL -1:\n",
    "            print(f\"[CompanySequenceModel] Epoch {epoch:03d} | \"\n",
    "                  f\"loss_cont={avg_loss_cont:.4f}  \"\n",
    "                  f\"loss_bin={avg_loss_bin:.4f}\")\n",
    "            \n",
    "            if writer_company:\n",
    "                for name, param in company_model.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        writer_company.add_histogram(f'Weights_Company/{name.replace(\".\", \"/\")}', param.data, epoch)\n",
    "    \n",
    "    if writer_company: writer_company.close() \n",
    "else:\n",
    "    print(\"Skipping CompanySequenceModel training as model or dataloader is not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c80b96",
   "metadata": {},
   "source": [
    "### 7.3 학습 완료 후 결과 수집 및 평탄화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a663fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_outputs = []\n",
    "all_year_outputs = [] \n",
    "all_stock_outputs = [] \n",
    "\n",
    "if company_model and dataloader_seq: # Check if model and dataloader are initialized\n",
    "    company_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_cont, batch_bin, batch_year, batch_stock, _ in dataloader_seq:\n",
    "            batch_cont = batch_cont.to(device)\n",
    "            batch_bin = batch_bin.to(device)\n",
    "            batch_year = batch_year.to(device)\n",
    "            batch_stock = batch_stock.to(device)\n",
    "            out = company_model(batch_cont, batch_bin, batch_year, batch_stock)  \n",
    "            all_outputs.append(out.cpu())\n",
    "            all_year_outputs.append(batch_year.cpu())\n",
    "            batch_stock_expanded = batch_stock.unsqueeze(1).repeat(1, NUM_YEARS_PARAM)\n",
    "            all_stock_outputs.append(batch_stock_expanded.cpu())\n",
    "\n",
    "    output_tensor_seq = torch.cat(all_outputs, dim=0)    \n",
    "    year_tensor_seq_output = torch.cat(all_year_outputs, dim=0)           \n",
    "    stock_tensor_seq_expanded = torch.cat(all_stock_outputs, dim=0) \n",
    "\n",
    "    output_tensor_flat = output_tensor_seq.reshape(-1, FT_OUT_DIM_PARAM)           \n",
    "    year_tensor_flat = year_tensor_seq_output.reshape(-1)  \n",
    "    year_embed_flat = get_sine_cosine_year_embedding(year_tensor_flat, dim=YEAR_DIM_PARAM) \n",
    "    stock_tensor_flat = stock_tensor_seq_expanded.reshape(-1)                  \n",
    "\n",
    "    print(f\"output_tensor_seq shape: {output_tensor_seq.shape}\")\n",
    "    print(f\"output_tensor_flat shape: {output_tensor_flat.shape}\")\n",
    "else:\n",
    "    print(\"Skipping CompanySequenceModel output collection as model or dataloader is not initialized.\")\n",
    "    output_tensor_seq = torch.empty(0, NUM_YEARS_PARAM, FT_OUT_DIM_PARAM) \n",
    "    year_tensor_seq_output = torch.empty(0, NUM_YEARS_PARAM)\n",
    "    stock_tensor_seq_expanded = torch.empty(0, NUM_YEARS_PARAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f67a6cd",
   "metadata": {},
   "source": [
    "## 8'. Transformer-Denoiser 기반 Diffusion (시계열 컨텍스트 활용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdb171e",
   "metadata": {},
   "source": [
    "### 8.1 Diffusion 하이퍼파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036d7d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_diff      = 10\n",
    "beta_start  = 1e-4\n",
    "beta_end    = 2e-2\n",
    "betas       = torch.linspace(beta_start, beta_end, T_diff, device=device)      \n",
    "alphas      = 1.0 - betas\n",
    "alpha_bars  = torch.cumprod(alphas, dim=0)                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdbbee0",
   "metadata": {},
   "source": [
    "### 8.2 학습/평가용 Dataset (Diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01e9ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_filtered.empty and NUM_STOCK_EMBEDDINGS > 0: # Ensure original data was processed\n",
    "    stock_scalar_seq_diff = torch.tensor(stock_seq, dtype=torch.long) \n",
    "    bin_label_tensor_seq_diff = torch.tensor(X_bin_seq, dtype=torch.float32) \n",
    "else: \n",
    "    stock_scalar_seq_diff = torch.empty(0, dtype=torch.long)\n",
    "    bin_label_tensor_seq_diff = torch.empty(0, NUM_YEARS_PARAM, bin_input_dim)\n",
    "\n",
    "if output_tensor_seq.numel() > 0: # Check if output_tensor_seq is not empty\n",
    "    diff_dataset = TensorDataset(\n",
    "        output_tensor_seq,          \n",
    "        year_tensor_seq_output,     \n",
    "        stock_scalar_seq_diff,      \n",
    "        bin_label_tensor_seq_diff   \n",
    "    )\n",
    "    diff_dataloader = DataLoader(diff_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    print(f\"Diffusion dataset size: {len(diff_dataset)}\")\n",
    "else:\n",
    "    print(\"Skipping Diffusion DataLoader creation as CompanySequenceModel output is empty.\")\n",
    "    diff_dataloader = [] # Or an empty DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330323ed",
   "metadata": {},
   "source": [
    "### 8.3 Sinusoidal 시간-스텝 임베딩 (`TimeEmbedding`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029677c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int):\n",
    "        super().__init__()\n",
    "        half_d_model = d_model // 2\n",
    "        inv_freq = 1. / (10000 ** (torch.arange(0, half_d_model, dtype=torch.float32) / half_d_model))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor: \n",
    "        sinusoid_input = t * self.inv_freq            \n",
    "        emb = torch.cat([torch.sin(sinusoid_input), torch.cos(sinusoid_input)], dim=-1)  \n",
    "        if self.d_model % 2 == 1:\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n",
    "        return emb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b5eda",
   "metadata": {},
   "source": [
    "### 8.4 Transformer-Denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d62c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDenoiser(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_stock_embeddings_denoiser, # Added parameter\n",
    "        feat_dim=FT_OUT_DIM_PARAM,         \n",
    "        d_model=DENOISER_D_MODEL,\n",
    "        nhead=4, num_layers=4,\n",
    "        stock_emb_dim=STOCK_DIM_PARAM,\n",
    "        year_pos_dim=YEAR_DIM_PARAM, \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.year_proj = nn.Linear(year_pos_dim, d_model) \n",
    "        self.stock_emb = nn.Embedding(num_stock_embeddings_denoiser, stock_emb_dim)\n",
    "        self.in_proj = nn.Linear(feat_dim + stock_emb_dim, d_model)\n",
    "\n",
    "        self.t_embed = nn.Sequential(\n",
    "            TimeEmbedding(d_model), nn.Linear(d_model, d_model), nn.SiLU()\n",
    "        )\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=NUM_YEARS_PARAM) \n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "\n",
    "        self.out_cont = nn.Linear(d_model, cont_input_dim)   \n",
    "        self.out_bin  = nn.Linear(d_model,  bin_input_dim)   \n",
    "\n",
    "    def forward(self, x_t, years, stock_id, t_norm):\n",
    "        B, S, _ = x_t.shape \n",
    "        year_embed_raw = get_sine_cosine_year_embedding(\n",
    "            years, dim=self.year_proj.in_features \n",
    "        ) \n",
    "        year_embed = self.year_proj(year_embed_raw) \n",
    "        stock_embed_val = self.stock_emb(stock_id).unsqueeze(1).repeat(1, S, 1)\n",
    "        h = self.in_proj(torch.cat([x_t, stock_embed_val], dim=-1)) \n",
    "        h = self.pos_enc(h) + year_embed + self.t_embed(t_norm).unsqueeze(1) \n",
    "        h = self.encoder(h) \n",
    "        return self.out_cont(h), self.out_bin(h)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7307f04a",
   "metadata": {},
   "source": [
    "### 8.5 Forward Diffusion (`q_sample`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1ce4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_sample(x0_seq, t_int):\n",
    "    t_idx = t_int.long() - 1 \n",
    "    sqrt_ab  = torch.sqrt(alpha_bars[t_idx]).view(-1,1,1)      \n",
    "    sqrt_1m_ab  = torch.sqrt(1-alpha_bars[t_idx]).view(-1,1,1) \n",
    "    noise    = torch.randn_like(x0_seq)\n",
    "    return sqrt_ab*x0_seq + sqrt_1m_ab*noise, noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f187b6d5",
   "metadata": {},
   "source": [
    "### 8.6 Denoiser 학습 설정 및 `snr_weight`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d386a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if NUM_STOCK_EMBEDDINGS > 0: # Ensure there are stocks to create embeddings for\n",
    "    denoiser = TransformerDenoiser(\n",
    "        num_stock_embeddings_denoiser=NUM_STOCK_EMBEDDINGS, # Pass the number of unique stocks\n",
    "        feat_dim=FT_OUT_DIM_PARAM, \n",
    "        d_model=DENOISER_D_MODEL, \n",
    "        year_pos_dim=YEAR_DIM_PARAM,\n",
    "        stock_emb_dim=STOCK_DIM_PARAM \n",
    "    ).to(device)\n",
    "    opt_denoiser = optim.AdamW(denoiser.parameters(), lr=LEARNING_RATE_DENOISER) # Renamed optimizer\n",
    "    current_time_str_denoiser = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    writer_denoiser = SummaryWriter(f'{TENSORBOARD_LOG_DIR_DENOISER}_{current_time_str_denoiser}')\n",
    "else:\n",
    "    print(\"Skipping Denoiser model initialization as there are no stocks.\")\n",
    "    denoiser = None\n",
    "    opt_denoiser = None\n",
    "    writer_denoiser = None\n",
    "\n",
    "def snr_weight(t_idx: torch.Tensor, \n",
    "               alpha_bars_local: torch.Tensor, \n",
    "               strategy: str = \"karras\",\n",
    "               rho: float = 1.2) -> torch.Tensor:\n",
    "    ab = alpha_bars_local[t_idx]                       \n",
    "    snr = ab / (1.0 - ab)\n",
    "    if strategy == \"karras\":                         \n",
    "        weight = (snr + 1.0).pow(-rho)\n",
    "    elif strategy == \"simple\":\n",
    "        weight = 1.0 / (snr + 1.0)\n",
    "    else:\n",
    "        raise ValueError(f\"unknown strategy {strategy}\")\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f1eab",
   "metadata": {},
   "source": [
    "### 8.7 Denoiser 학습 루프"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e852fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_c = nn.MSELoss(reduction='none')                   \n",
    "bce_fn      = nn.BCEWithLogitsLoss(pos_weight=pos_w,         \n",
    "                                   reduction='none')\n",
    "\n",
    "λ_bin_denoiser = 10.0   \n",
    "\n",
    "if denoiser and diff_dataloader: # Check if model and dataloader are initialized\n",
    "    if writer_denoiser: # Check if writer is initialized\n",
    "        try:\n",
    "            data_iter_denoiser = iter(diff_dataloader) \n",
    "            sample_x0_diff, sample_yrs_diff, sample_st_diff, _ = next(data_iter_denoiser)\n",
    "            sample_t_norm = torch.rand(sample_x0_diff.size(0), 1, device=device) \n",
    "            writer_denoiser.add_graph(denoiser, [sample_x0_diff.to(device), sample_yrs_diff.to(device), sample_st_diff.to(device), sample_t_norm])\n",
    "            del data_iter_denoiser \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding Denoiser graph to TensorBoard: {e}\")\n",
    "\n",
    "    for ep in range(EPOCHS_DENOISER_MODEL):\n",
    "        epoch_loss_denoiser = 0.0\n",
    "        num_batches_denoiser = 0\n",
    "        for x0_diff, yrs_diff, st_diff, bin_true_diff in diff_dataloader:\n",
    "            x0_diff, yrs_diff, st_diff = x0_diff.to(device), yrs_diff.to(device), st_diff.to(device)\n",
    "            bin_true_diff = bin_true_diff.to(device)\n",
    "            B = x0_diff.size(0)\n",
    "\n",
    "            t_int_rand, _ = torch.sort(torch.randint(1, T_diff + 1, (B,), device=device))\n",
    "            x_t, _   = q_sample(x0_diff, t_int_rand)                 \n",
    "            t_norm   = t_int_rand.float().unsqueeze(1) / T_diff \n",
    "\n",
    "            cont_hat, bin_hat = denoiser(x_t, yrs_diff, st_diff, t_norm)   \n",
    "            bin_hat = bin_hat.clamp(-15, 15) \n",
    "\n",
    "            cont_tgt = x0_diff[:, :, :cont_input_dim] \n",
    "            bin_tgt  = bin_true_diff \n",
    "\n",
    "            mse = criterion_c(cont_hat, cont_tgt).mean(dim=(1,2))          \n",
    "            bce = bce_fn(bin_hat, bin_tgt).mean(dim=(1, 2))              \n",
    "            w   = snr_weight(t_int_rand - 1, alpha_bars, \"karras\", rho=1.2)       \n",
    "            loss = (w * mse + λ_bin_denoiser * bce).mean() \n",
    "\n",
    "            opt_denoiser.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            if writer_denoiser and (ep % (EPOCHS_DENOISER_MODEL // 5 if EPOCHS_DENOISER_MODEL >=5 else 1) == 0 or ep == EPOCHS_DENOISER_MODEL -1): \n",
    "                for name, param in denoiser.named_parameters():\n",
    "                    if param.grad is not None:\n",
    "                        writer_denoiser.add_histogram(f'Gradients_Denoiser/{name.replace(\".\", \"/\")}', param.grad, ep)\n",
    "            \n",
    "            opt_denoiser.step()\n",
    "            \n",
    "            epoch_loss_denoiser += loss.item()\n",
    "            num_batches_denoiser +=1\n",
    "\n",
    "        avg_loss_denoiser = epoch_loss_denoiser / num_batches_denoiser if num_batches_denoiser > 0 else 0\n",
    "        if writer_denoiser:\n",
    "            writer_denoiser.add_scalar('Loss_Denoiser/Total_Train', avg_loss_denoiser, ep)\n",
    "\n",
    "        if ep % (EPOCHS_DENOISER_MODEL // 5 if EPOCHS_DENOISER_MODEL >=5 else 1) == 0 or ep == EPOCHS_DENOISER_MODEL -1:\n",
    "            print(f\"[Denoiser] ep {ep:03d} | loss {avg_loss_denoiser:.5f}\")\n",
    "            \n",
    "            if writer_denoiser:\n",
    "                for name, param in denoiser.named_parameters():\n",
    "                    if param.requires_grad:\n",
    "                        writer_denoiser.add_histogram(f'Weights_Denoiser/{name.replace(\".\", \"/\")}', param.data, ep)\n",
    "    \n",
    "    if writer_denoiser: writer_denoiser.close() \n",
    "else:\n",
    "    print(\"Skipping Denoiser training as model or dataloader is not initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63812d1f",
   "metadata": {},
   "source": [
    "### 8.8 Reverse Diffusion Sampler (`p_sample_loop`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d7f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def p_sample_loop(model, years_vec, stock_id, seq_len=NUM_YEARS_PARAM):\n",
    "    model.eval() \n",
    "    x = torch.randn(1, seq_len, model.feat_dim, device=device) \n",
    "    years = years_vec.unsqueeze(0).to(device)         \n",
    "    stock = torch.tensor([stock_id], device=device)   \n",
    "\n",
    "    for t_val in range(T_diff, 0, -1): \n",
    "        t_norm_sample = torch.full((1, 1), t_val / T_diff, device=device)\n",
    "        t_idx_current = t_val - 1 \n",
    "\n",
    "        cont_hat, bin_hat = model(x, years, stock, t_norm_sample)  \n",
    "        x0_hat = torch.cat([cont_hat, bin_hat], dim=-1)     \n",
    "\n",
    "        alpha_bar_t = alpha_bars[t_idx_current]\n",
    "        eps_hat = (x - alpha_bar_t.sqrt() * x0_hat) / torch.sqrt(1 - alpha_bar_t)\n",
    "\n",
    "        beta_t, alpha_t = betas[t_idx_current], alphas[t_idx_current]\n",
    "        mean = (1 / alpha_t.sqrt()) * (x - beta_t * eps_hat / torch.sqrt(1 - alpha_bar_t))\n",
    "\n",
    "        if t_val > 1:\n",
    "            x = mean + beta_t.sqrt() * torch.randn_like(x)\n",
    "        else:\n",
    "            x = mean                                    \n",
    "\n",
    "    cont_final, bin_logit = x[:, :, :cont_input_dim], x[:, :, cont_input_dim:]\n",
    "    bin_prob = torch.sigmoid(bin_logit)\n",
    "    bin_final = (bin_prob > 0.5).float()\n",
    "\n",
    "    x0_final = torch.cat([cont_final, bin_final], dim=-1)   \n",
    "    return x0_final.squeeze(0)                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9eb057",
   "metadata": {},
   "source": [
    "### 8.9 Inverse Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00383120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(data_np: np.ndarray, scaler_cont) -> np.ndarray: \n",
    "    data_np_transformed = data_np.copy()\n",
    "    data_np_transformed[:, :cont_input_dim] = 1.0 / (1.0 + np.exp(-data_np_transformed[:, :cont_input_dim])) \n",
    "    if data_np_transformed[:, :cont_input_dim].size > 0: \n",
    "        data_np_transformed[:, :cont_input_dim] = scaler_cont.inverse_transform(data_np_transformed[:, :cont_input_dim]) \n",
    "    data_np_transformed[:, cont_input_dim:] = data_np_transformed[:, cont_input_dim:].astype(int)\n",
    "    return data_np_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218adb25",
   "metadata": {},
   "source": [
    "## 9. 대량 기업 생성 & CSV 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d00041a",
   "metadata": {},
   "source": [
    "### 9.1 `generate_synthetic_companies` 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf1074f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_companies(model, scaler_cont_features, num_companies_to_gen=500, \n",
    "                                 seq_len=NUM_YEARS_PARAM, start_vid=10000):\n",
    "    if model is None:\n",
    "        print(\"Denoiser model is not initialized. Skipping synthetic data generation.\")\n",
    "        return np.empty((0, 2 + cont_input_dim + bin_input_dim))\n",
    "        \n",
    "    model.eval()\n",
    "    n_stock_real = NUM_STOCK_EMBEDDINGS if NUM_STOCK_EMBEDDINGS > 0 else 1\n",
    "        \n",
    "    all_rows = []\n",
    "    years_vec_gen = torch.arange(2011, 2011+seq_len, dtype=torch.float32)\n",
    "\n",
    "    for i in range(num_companies_to_gen):\n",
    "        virt_id   = start_vid + i\n",
    "        stock_real_id = virt_id % n_stock_real \n",
    "        \n",
    "        x_gen     = p_sample_loop(model, years_vec_gen, stock_real_id, seq_len)\n",
    "        x_np      = inverse_transform(x_gen.cpu().numpy(), scaler_cont_features)\n",
    "        rows      = np.hstack([\n",
    "                      np.full((seq_len,1), virt_id),\n",
    "                      years_vec_gen.reshape(-1,1).numpy(), \n",
    "                      x_np\n",
    "                    ])\n",
    "        all_rows.append(rows)\n",
    "        if (i+1) % (num_companies_to_gen // 10 if num_companies_to_gen >=10 else 1) == 0 or i == num_companies_to_gen -1:\n",
    "            print(f\"  • {i+1}/{num_companies_to_gen} synthetic companies generated\")\n",
    "\n",
    "    if not all_rows:\n",
    "        return np.empty((0, 2 + cont_input_dim + bin_input_dim))\n",
    "    return np.vstack(all_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224855c9",
   "metadata": {},
   "source": [
    "### 9.2 평가: 이진 변수 확률 히스토그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075da97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if denoiser and diff_dataloader and len(diff_dataloader) > 0:\n",
    "    denoiser.eval()\n",
    "    all_prob = []       \n",
    "    with torch.no_grad():\n",
    "        for x0_eval, yrs_eval, st_eval, _ in diff_dataloader: \n",
    "            x_t_eval, _ = q_sample(x0_eval.to(device),\n",
    "                              torch.ones(len(x0_eval), device=device, dtype=torch.long))  \n",
    "            _, bin_hat_eval = denoiser(\n",
    "                x_t_eval,\n",
    "                yrs_eval.to(device),\n",
    "                st_eval.to(device),\n",
    "                torch.ones(len(x0_eval), 1, device=device) / T_diff \n",
    "            )\n",
    "            prob = torch.sigmoid(bin_hat_eval).cpu().numpy().ravel()    \n",
    "            all_prob.append(prob)\n",
    "\n",
    "    all_prob = np.concatenate(all_prob)          \n",
    "    hist, bin_edges = np.histogram(all_prob, bins=10, range=(0.0, 1.0))\n",
    "\n",
    "    print(\"\\nProbabilities histogram for binary features (0~1):\")\n",
    "    for i in range(10):\n",
    "        print(f\"{bin_edges[i]:.1f}–{bin_edges[i+1]:.1f}: {hist[i]}\")\n",
    "else:\n",
    "    print(\"Skipping probability histogram generation as Denoiser model or diff_dataloader is not initialized or empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fc23e1",
   "metadata": {},
   "source": [
    "### 9.3 평가: `positive_ratio` 함수 및 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9854e6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def positive_ratio(model, dataloader, threshold=0.5):\n",
    "    if model is None or not dataloader:\n",
    "        print(\"Skipping positive_ratio calculation as model or dataloader is not initialized or empty.\")\n",
    "        return\n",
    "        \n",
    "    model.eval()\n",
    "    n_pred_pos = torch.zeros(bin_input_dim, device=device)   \n",
    "    n_true_pos = torch.zeros(bin_input_dim, device=device)\n",
    "    n_total    = 0\n",
    "\n",
    "    for x0_pr, yrs_pr, st_pr, bin_true_pr in dataloader:\n",
    "        x0_pr, yrs_pr, st_pr = x0_pr.to(device), yrs_pr.to(device), st_pr.to(device)\n",
    "        bin_true_pr = bin_true_pr.to(device)\n",
    "        B, S, _ = x0_pr.shape \n",
    "\n",
    "        t_int_pr  = torch.ones(B, device=device, dtype=torch.long) \n",
    "        x_t_pr, _ = q_sample(x0_pr, t_int_pr)\n",
    "        t_norm_pr = t_int_pr.float().unsqueeze(1) / T_diff\n",
    "\n",
    "        _, bin_logit_pr = model(x_t_pr, yrs_pr, st_pr, t_norm_pr)        \n",
    "        prob_pr = torch.sigmoid(bin_logit_pr)                   \n",
    "        pred_pos_pr = (prob_pr > threshold).sum(dim=(0, 1))     \n",
    "        true_pos_pr = (bin_true_pr > 0.5).sum(dim=(0,1))\n",
    "        \n",
    "        n_pred_pos += pred_pos_pr\n",
    "        n_true_pos += true_pos_pr\n",
    "        n_total    += B * S\n",
    "\n",
    "    ratio_pred = (n_pred_pos.cpu() / n_total).numpy() if n_total > 0 else np.zeros(bin_input_dim)\n",
    "    ratio_true = (n_true_pos.cpu() / n_total).numpy() if n_total > 0 else np.zeros(bin_input_dim)\n",
    "    diff       = ratio_pred - ratio_true\n",
    "\n",
    "    print(f\"\\n★ Positive-ratio check (thr={threshold})\")\n",
    "    for i, name in enumerate(binary_features):\n",
    "        print(f\" {name:5s}  pred={ratio_pred[i]:.3%}  \"\n",
    "              f\"true={ratio_true[i]:.3%}  diff={diff[i]:+.2%}\")\n",
    "    print()\n",
    "\n",
    "positive_ratio(denoiser, diff_dataloader, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3998d88",
   "metadata": {},
   "source": [
    "### 9.4 최종 데이터 생성 및 CSV 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e466f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n▶  가짜 기업 생성 시작 …\")\n",
    "if not df_filtered.empty and minmax_scaler is not None and denoiser is not None: \n",
    "    fake_data = generate_synthetic_companies(denoiser, minmax_scaler, num_companies_to_gen=500)\n",
    "    \n",
    "    if fake_data.shape[0] > 0: # Check if fake_data is not empty\n",
    "        raw_cols = [\"Stock_ID\", \"YEAR\"] + continuous_features + binary_features\n",
    "        df_fake  = pd.DataFrame(fake_data, columns=raw_cols)\n",
    "\n",
    "        final_cols_order = [\"OWN\", \"FORN\", \"BIG4\",\"SIZE\", \"LEV\", \"CUR\", \"GRW\", \"ROA\",\n",
    "                      \"ROE\",\"CFO\", \"PPE\", \"AGE\", \"INVREC\", \"MB\", \"TQ\", \"LOSS\"]\n",
    "        df_fake = df_fake[[\"Stock_ID\", \"YEAR\"] + final_cols_order] \n",
    "\n",
    "        df_fake[\"Stock_ID\"] = df_fake[\"Stock_ID\"].astype(int)\n",
    "        df_fake[\"YEAR\"]     = df_fake[\"YEAR\"].astype(int)\n",
    "\n",
    "        for col in final_cols_order:\n",
    "            if col in df_fake.columns:\n",
    "                 if df_fake[col].dtype == 'float64' or df_fake[col].dtype == 'float32':\n",
    "                    df_fake[col] = df_fake[col].round(8)\n",
    "\n",
    "        output_csv_path = \"generated_synthetic_companies.csv\"\n",
    "        df_fake.to_csv(output_csv_path, index=False)\n",
    "        print(f\"✅ {len(df_fake)//NUM_YEARS_PARAM}개 기업 × {NUM_YEARS_PARAM}년 시계열 저장 완료: {output_csv_path}\")\n",
    "        display(df_fake.head()) # Use display for better notebook output\n",
    "    else:\n",
    "        print(\"No fake data was generated.\")\n",
    "else:\n",
    "    print(\"Skipping fake data generation and saving as original data was empty, scaler not fitted, or denoiser model not initialized.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "copy_financial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
